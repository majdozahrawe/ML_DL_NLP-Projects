{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-_AjjKEyCpS"
      },
      "outputs": [],
      "source": [
        "!pip install telethon\n",
        "!pip install nest_asyncio\n",
        "!pip install openpyxl\n",
        "!pip install telethon nest_asyncio openpyxl\n",
        "!pip install telethon nest_asyncio openpyxl nltk transformers sentence-transformers\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RqlaIcnh-WB"
      },
      "source": [
        "Posts and Comments Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AhCPKSy2ZnU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "from telethon import TelegramClient\n",
        "from telethon.errors import MsgIdInvalidError\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "nest_asyncio.apply()\n",
        "get them from telegram.\n",
        "api_id = 28524658\n",
        "api_hash = '5117388049357ea47ab79365e82750ae'\n",
        "create session to save login informations.\n",
        "client = TelegramClient('YD_Cryptocurrency', api_id, api_hash)\n",
        "Here define the date scope of posts\n",
        "start_date = datetime(2023, 1, 15, 1, 9, 14, tzinfo=timezone.utc)\n",
        "end_date = datetime(2025, 2, 17, 12, 8, 7, tzinfo=timezone.utc)\n",
        "Function to fetch and count post emoji reactions\n",
        "\n",
        "def count_post_reactions(reactions):\n",
        "    counts = {\n",
        "        'Heart': 0, 'Thumbs Up': 0, 'Thumbs Down': 0,\n",
        "        'Clapping Hands': 0, 'Exploding Head': 0, 'Broken Heart': 0\n",
        "    }\n",
        "\n",
        "    emoji_map = {\n",
        "        '❤': 'Heart', '👍': 'Thumbs Up', '👎': 'Thumbs Down',\n",
        "        '👏': 'Clapping Hands', '🤯': 'Exploding Head', '💔': 'Broken Heart'\n",
        "    }\n",
        "\n",
        "    if reactions:\n",
        "        for reaction in reactions.results:\n",
        "            emoji = getattr(reaction.reaction, 'emoticon', None) or getattr(reaction.reaction, 'emoji', None)\n",
        "            if emoji in emoji_map:\n",
        "                counts[emoji_map[emoji]] += reaction.count\n",
        "\n",
        "    return counts\n",
        "\n",
        "function to fetch and count comment emoji reactions.\n",
        "\n",
        "def count_comment_reactions(reactions):\n",
        "    counts = {\n",
        "        'Heart': 0, 'Thumbs Up': 0\n",
        "    }\n",
        "\n",
        "    emoji_map = {\n",
        "        '❤': 'Heart', '👍': 'Thumbs Up'\n",
        "    }\n",
        "    if reactions:\n",
        "        for reaction in reactions.results:\n",
        "            emoji = getattr(reaction.reaction, 'emoticon', None) or \\\n",
        "                    getattr(reaction.reaction, 'emoji', None)\n",
        "            if emoji == '❤':\n",
        "                counts['Heart'] += reaction.count\n",
        "            # elif emoji == '🙏':\n",
        "            #     counts['Pray'] += reaction.count\n",
        "            elif emoji == '👍':\n",
        "                counts['Thumbs Up'] += reaction.count\n",
        "            # elif emoji == '🥰':\n",
        "            #     counts['Face with Hearts'] += reaction.count\n",
        "\n",
        "    return counts\n",
        "\n",
        "this function used when the post has no comments.\n",
        "Function to fetch all data from the Telegram channel\n",
        "this main function used when work on colab or jupyter notebook to run the client and fetch data\n",
        "async def fetch_comments_for_posts(posts, channel):\n",
        "    tasks = [client.get_messages(channel, reply_to=post.id, limit=100) for post in posts]\n",
        "    return await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "async def fetch_data_with_comments(channel_username, batch_size=50, max_messages=400):\n",
        "    channel = await client.get_entity(channel_username)\n",
        "    posts = await fetch_data_from_channel(channel, batch_size=batch_size, max_messages=max_messages)\n",
        "\n",
        "    print(\"Fetching comments for all posts...\")\n",
        "    comments_data = await fetch_comments_for_posts(posts, channel)\n",
        "\n",
        "    structured_data = []\n",
        "    for post, comments in zip(posts, comments_data):\n",
        "        if post.text:\n",
        "            post_content = post.text.strip('**')\n",
        "            word_count = len(post_content.split())\n",
        "            avg_words_per_sentence = word_count / max(1, post_content.count('.') + post_content.count('!') + post_content.count('?'))\n",
        "            post_reactions = count_post_reactions(post.reactions)\n",
        "\n",
        "            comments_details = []\n",
        "            if isinstance(comments, list):\n",
        "                for comment in comments:\n",
        "                    if comment.text:\n",
        "                        comment_reactions = count_comment_reactions(comment.reactions)\n",
        "                        comments_details.append({\n",
        "                            'Comment ID': comment.id,\n",
        "                            'Comment Text': comment.text,\n",
        "                            'Comment Time': comment.date.isoformat(),\n",
        "                            **comment_reactions\n",
        "                        })\n",
        "\n",
        "            structured_data.append({\n",
        "                'Post ID': post.id,\n",
        "                'Post Content': post_content,\n",
        "                'Post Time': post.date.isoformat(),\n",
        "                'Views': post.views or 0,\n",
        "                'Forwards': post.forwards or 0,\n",
        "                **post_reactions,\n",
        "                'Word Count': word_count,\n",
        "                'Average Words per Sentence': avg_words_per_sentence,\n",
        "                'Comments Details': comments_details\n",
        "            })\n",
        "\n",
        "    print(f\"✅ Total structured posts fetched: {len(structured_data)}\")\n",
        "    return structured_data\n",
        "\n",
        "def save_to_excel(data, filename='ناشيونال جيوغرافيگ.xlsx'):\n",
        "    rows = []\n",
        "    for post in data:\n",
        "        if 'Comments Details' not in post or not post['Comments Details']:\n",
        "            post['Comments Details'] = [{'Comment ID': None, 'Comment Text': None, 'Comment Time': None,\n",
        "                                         'Heart': 0,\n",
        "                                         'Thumbs Up': 0,}]\n",
        "\n",
        "        for comment in post['Comments Details']:\n",
        "            rows.append({\n",
        "                'Post ID': post['Post ID'],\n",
        "                'Post Content': post['Post Content'],\n",
        "                'Post Time': post['Post Time'],\n",
        "                'Views': post['Views'],\n",
        "                'Forwards': post['Forwards'],\n",
        "                **{key + ' (Post)': post.get(key, 0) for key in ['Heart', 'Thumbs Up', 'Thumbs Down', 'Clapping Hands', 'Exploding Head', 'Broken Heart']},\n",
        "                'Word Count': post['Word Count'],\n",
        "                'Average Words per Sentence': post['Average Words per Sentence'],\n",
        "                'Comment ID': comment['Comment ID'],\n",
        "                'Comment Text': comment['Comment Text'],\n",
        "                'Comment Time': comment['Comment Time'],\n",
        "                **{key + ' (Comment)': comment.get(key, 0) for key in ['Thumbs Up','Heart']}\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_excel(filename, index=False, engine='openpyxl')\n",
        "    print(f\"✅ Data successfully saved to {filename}\")\n",
        "\n",
        "async def main():\n",
        "    async with client:\n",
        "        data = await fetch_data_with_comments('@asbwf')  # ✅ Fetch data\n",
        "        save_to_excel(data)  # ✅ Save as Excel (.xlsx)\n",
        "\n",
        "asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrvFFTtuzalg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "datasets = [\n",
        "    \"Youcef YD - العملات الرقميه 🏆🏅.xlsx\",\n",
        "    \"أهداف المباريات _ YSM Sports.xlsx\",\n",
        "    \"برمجة وتطوير.xlsx\",\n",
        "    \"عصير الكتب - PDF.xlsx\",\n",
        "    \"ناشيونال جيوغرافيگ.xlsx\"\n",
        "]\n",
        "\n",
        "frames = [pd.read_excel(file) for file in datasets]\n",
        "\n",
        "merged_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "\n",
        "output_path = \"merged_dataset.xlsx\"\n",
        "merged_df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Merged dataset saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI1-_TOVy8vC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'modified_dataset.xlsx'\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "df = xls.parse(xls.sheet_names[0])\n",
        "\n",
        "post_columns = [\n",
        "    'Post ID', 'Post Content', 'Post Time', 'Views', 'Forwards', 'Fire (Post)',\n",
        "    'Saluting Face (Post)', 'Pray (Post)', 'Trophy (Post)', 'Clapping (Post)',\n",
        "    'Party (Post)', 'Face with Hearts (Post)', 'Thumbs Up (Post)', 'Heart (Post)',\n",
        "    'Word Count', 'Average Words per Sentence'\n",
        "]\n",
        "\n",
        "\n",
        "comment_columns = [\n",
        "  'Post ID','Comment ID', 'Comment Text', 'Comment Time', 'Heart (Comment)', 'Thumbs Up (Comment)'\n",
        "]\n",
        "\n",
        "posts_df = df[post_columns].drop_duplicates(subset=['Post ID'])\n",
        "comments_df = df[comment_columns].dropna(subset=['Comment ID'])\n",
        "\n",
        "posts_file = 'posts.xlsx'\n",
        "comments_file = 'commentsID.xlsx'\n",
        "\n",
        "comments_df.to_excel(comments_file, index=False)\n",
        "\n",
        "print(f\"Posts saved to: {posts_file}\")\n",
        "print(f\"Comments saved to: {comments_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI-pYJNQiDRv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "file_path = \"posts.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "text_column = \"Post Content\" if \"Post Content\" in df.columns else df.columns[0]\n",
        "\n",
        "narrative_keywords = {\n",
        "    \"Engage\": [\"يا مغيث\", \"نداء\", \"الغضب\", \"التفاعل\", \"معركة\"],\n",
        "    \"Explain\": [\"معاني\", \"سبب\", \"شرح\", \"تفاصيل\", \"أسباب\", \"توضيح\", \"معنى\"],\n",
        "    \"Excite\": [\"حماس\", \"فرح\", \"مجد\", \"نصر\", \"افتخار\"],\n",
        "    \"Enhance\": [\"دعم\", \"تشجيع\", \"تعزيز\", \"وعي\", \"توعية\", \"تحفيز\"],\n",
        "    \"Dismiss\": [\"مبالغ\", \"تضخيم\", \"تافه\", \"ليس مهم\", \"وهم\", \"فقاعة\", \"تفاهة\"],\n",
        "    \"Distort\": [\"تحريف\", \"تشويه\", \"تضليل\", \"كذب\", \"خداع\"],\n",
        "    \"Dismay\": [\"قلق\", \"حزن\", \"رعب\", \"ألم\", \"دموع\"],\n",
        "    \"Distract\": [\"غير ذي صلة\", \"تحويل الأنظار\", \"موضوع آخر\", \"بعيد عن\", \"لا يرتبط\"]\n",
        "}\n",
        "\n",
        "def normalized_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "def classify_narrative(content):\n",
        "    if pd.isna(content) or not isinstance(content, str) or content.strip() == \"\":\n",
        "        return None\n",
        "    content = normalized_text(content)\n",
        "    for label, keywords in narrative_keywords.items():\n",
        "        if any(keyword in content for keyword in keywords):\n",
        "            return label\n",
        "    return \"Natural\"\n",
        "\n",
        "df[\"Narrative Manipulation\"] = df[text_column].apply(classify_narrative)\n",
        "\n",
        "output_path = \"narrative_manipulation_posts.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Narrative manipulation analysis complete. Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwHEPEauMsR0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"narrative_manipulation_posts.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "against_keywords = [\n",
        "    \"الغاشم\", \"لا يمكن\", \"عنيف\", \"العدوان\",\n",
        "    \"تحذير\", \"خطر\", \"مشكلة\", \"انتقاد\", \"سلبي\", \"سقوط\",\"سلبي\",\"تقليل\",\"تخفيض\"\n",
        "    \"هجوم\", \"معارضة\", \"احتجاج\", \"تهديد\", \"تحريض\", \"معاداة\", \"خسارة\"\n",
        "]\n",
        "\n",
        "favor_keywords = [\n",
        "    \"انتصار\", \"دعم\", \"نجاح\", \"إشادة\", \"موافقة\", \"احتفال\", \"تأييد\",\"تزويد\",\"سرعة\",\n",
        "    \"مساندة\", \"حفل\", \"فرح\", \"نشر\", \"تمكين\", \"معقول\", \"فوز\", \"خدمة\", \"يخدم\", \"يدعم\",\"ايجابي\",\"إيجابي\",\"وووو\",\"يييي\"\n",
        "]\n",
        "\n",
        "def label_post(row):\n",
        "    content = str(row['Post Content'])\n",
        "    content_lower = content.lower() if content != 'nan' else \"\"\n",
        "\n",
        "    # down_broken_reactions = row.get('Thumbs Down (Post)', 0) + row.get('Broken Heart (Post)', 0)\n",
        "    # heart_thumbs_reactions = row.get('Heart (Post)', 0) + row.get('Thumbs Up (Post)', 0)\n",
        "\n",
        "    if any(keyword in content_lower for keyword in against_keywords):\n",
        "            return \"Against\"\n",
        "    elif any(keyword in content_lower for keyword in favor_keywords):\n",
        "            return \"Favor\"\n",
        "\n",
        "    return \"Natural\"\n",
        "\n",
        "df['Label'] = df.apply(label_post, axis=1)\n",
        "\n",
        "output_path = \"labeled_narrative_manipulation_posts.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrg8l1yQt65e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "file_path = 'classified_eppm_posts.xlsx'\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "post_ids = df['Post ID']\n",
        "post_contents = df['Post Content'].fillna('').str.lower()\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(post_contents)\n",
        "\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "similar_posts = []\n",
        "for i in range(cosine_sim_matrix.shape[0]):\n",
        "    for j in range(i + 1, cosine_sim_matrix.shape[1]):\n",
        "        if cosine_sim_matrix[i, j] >= similarity_threshold:\n",
        "            similar_posts.append({\n",
        "                'Post ID 1': post_ids[i],\n",
        "                'Post ID 2': post_ids[j],\n",
        "                'Similarity Score': cosine_sim_matrix[i, j],\n",
        "                'Post1 Content': post_contents[i],\n",
        "                'Post2 Content': post_contents[j]\n",
        "            })\n",
        "\n",
        "similar_posts_df = pd.DataFrame(similar_posts)\n",
        "\n",
        "if not similar_posts_df.empty:\n",
        "    output_file_path = 'similar_posts_results_full_content.xlsx'\n",
        "    similar_posts_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"Results saved to {output_file_path}\")\n",
        "else:\n",
        "    print(\"No highly similar posts detected based on the chosen threshold.\")\n",
        "\n",
        "if similar_posts:\n",
        "    similarity_scores = [entry['Similarity Score'] for entry in similar_posts]\n",
        "    min_similarity = min(similarity_scores)\n",
        "    max_similarity = max(similarity_scores)\n",
        "    avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "    print(f\"Min: {min_similarity:.2f}, Max: {max_similarity:.2f}, Avg: {avg_similarity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffSck_0mBtA_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "main_file_path = 'classified_eppm_posts.xlsx'\n",
        "df_main = pd.read_excel(main_file_path, sheet_name='Sheet1')\n",
        "\n",
        "similarity_file_path = 'similar_posts_results_full_content.xlsx'\n",
        "df_similarity = pd.read_excel(similarity_file_path)\n",
        "\n",
        "most_similar_dict = {}\n",
        "\n",
        "for _, row in df_similarity.iterrows():\n",
        "    post1, post2, sim_score = row['Post ID 1'], row['Post ID 2'], row['Similarity Score']\n",
        "\n",
        "    if post1 not in most_similar_dict or sim_score > most_similar_dict[post1][1]:\n",
        "        most_similar_dict[post1] = (post2, sim_score)\n",
        "\n",
        "    if post2 not in most_similar_dict or sim_score > most_similar_dict[post2][1]:\n",
        "        most_similar_dict[post2] = (post1, sim_score)\n",
        "\n",
        "df_main['Most Similar Post ID'] = df_main['Post ID'].map(lambda x: most_similar_dict.get(x, (None, None))[0])\n",
        "df_main['Highest Similarity Score'] = df_main['Post ID'].map(lambda x: most_similar_dict.get(x, (None, None))[1])\n",
        "\n",
        "df_main['Most Similar Post ID'].fillna('No Match', inplace=True)\n",
        "df_main['Highest Similarity Score'].fillna(0, inplace=True)\n",
        "\n",
        "output_file = 'updated_post_narrative_with_similarity.xlsx'\n",
        "df_main.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8stP0cVV9KI_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'labeled_narrative_manipulation_posts.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def elaboration_likelihood_model(content):\n",
        "    if pd.isna(content):\n",
        "        return \"Other\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"تصريحات\", \"التصريحات\", \"تصريح\", \"بيان\", \"أخبار\", \"التصريح\", \"معلومات\", \"الرسالة\", \"التفاصيل\", \"الحق\", \"يشرح\", \"كيف\", \"ما معنى\"]):\n",
        "        return \"Information\"\n",
        "    elif any(keyword in content for keyword in [\"?\", \"هل\", \"ماذا لو\", \"??\", \"؟؟\", \"؟\", \"!\", \"!!\"]):\n",
        "        return \"Question\"\n",
        "    elif any(keyword in content for keyword in [\"دعم\", \"تتبرع\", \"مظاهرة\", \"شارك\", \"بمشاركة\", \"تبرع\", \"والتبرع\", \"المشاركة\", \"دعما\", \"التبرع\", \"لتبرع\", \"مشاركته\", \"بتبرع\", \"التبرعات\", \"للمشاركة\", \"ودعما\", \"بالتبرع\", \"الدعم\", \"يدعم\", \"يشارك\", \"يساند\", \"اسناد\", \"إسناد\", \"دعماً\", \"مشاركة\"]):\n",
        "        return \"Participation\"\n",
        "    elif any(keyword in content for keyword in [\"زعيم\", \"لقائد\", \"سياسي\", \"قائد\", \"قائدنا\", \"القائد\", \"قادة\", \"القادة\", \"للقائد\", \"السياسي\", \"السياسية\", \"مشاهير\", \"الرئيس\", \"السنوار\", \"ابو عبيدة\", \"حزب الله\", \"القسام\", \"حماس\", \"نصرالله\"]):\n",
        "        return \"Celebrity\"\n",
        "    elif any(keyword in content for keyword in [\"نصرة\", \"نصرالله\", \"دعاء\", \"نصر\", \"إلهام\", \"النصر\", \"أمل\", \"اماني\", \"أماني\", \"المفاجأة\"]):\n",
        "        return \"Inspiration\"\n",
        "    elif any(keyword in content for keyword in [\"ضحك\", \"مزاح\", \"سخرية\", \"المضحك\", \"للسخرية\", \"بتمسخرو\", \"ههههههه\", \"ههههه\", \"ha\", \"هههههههههههه\", \"ضحكتني\", \"مسخرة\", \"ههه\", \"هههههههههههههههه\"]):\n",
        "        return \"Humor/Sarcasm\"\n",
        "    elif any(keyword in content for keyword in [\"ومشاهد\", \"شهادة\", \"الشهادة\", \"المشاهد\", \"رواية\", \"شاهد\", \"مشاهد\", \"قصة\", \"مشهد\", \"الحلم\", \"رواية\"]):\n",
        "        return \"Anecdotal/Story\"\n",
        "    else:\n",
        "        return \"Information\"\n",
        "\n",
        "df['elaboration_likelihood'] = df['Post Content'].apply(elaboration_likelihood_model)\n",
        "\n",
        "output_file = 'categorized_posts.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "428QTcna-PG0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def social_judgment_theory(content):\n",
        "    if pd.isna(content):\n",
        "        return \"\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"دواء\", \"صحة\", \"مرض\", \"شفاء\", \"علاج\",\"طب\",\"تمريض\",\"اصابة\"]):\n",
        "        return \"Health/Evidence-based Health Information\"\n",
        "    elif any(keyword in content for keyword in [\"النبي\", \"القرآن\", \"الإسلام\", \"دين\", \"صلاة\", \"الدين\", \"اليهودية\",\n",
        "                                                \"الجنة\", \"اليهود\", \"مسلمين\", \"مسلمون\", \"إسلام\",\"المسيحية\"]):\n",
        "        return \"Religion\"\n",
        "    elif any(keyword in content for keyword in [\"اختيار\", \"قرار\", \"حرية\", \"نظام\", \"حقوق\", \"رفاهية\",\"امكانية\",\"خيار\"]):\n",
        "        return \"Choice\"\n",
        "    elif any(keyword in content for keyword in [\"حكومة\", \"سياسي\", \"احتلال\", \"دولة\", \"جيش\", \"ثورة\", \"الدولة\",\n",
        "                                                \"سياسة\", \"السياسة\", \"المعتقلين\",\"ترامب\",\"بايدن\",\"دولي\"]):\n",
        "        return \"Political\"\n",
        "    elif any(keyword in content for keyword in [\"الله\",\"تضحية\", \"إيثار\", \"كرم\", \"نصر\", \"دعم\"]):\n",
        "        return \"Altruism\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['social_judgment'] = df['Post Content'].apply(social_judgment_theory)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset3.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnpD6DvD_u68"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset3.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def classify_eppm(content):\n",
        "    if pd.isna(content):\n",
        "        return \"Other\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\n",
        "        \"خطر\", \"كارثة\", \"تهديد\", \"خسارة\", \"دمار\", \"مأساة\", \"موت\", \"وفيات\",\n",
        "        \"قصف\", \"دماء\", \"حزن\", \"عذاب\", \"معاناة\", \"كارثة\", \"مجزرة\", \"خراب\",\n",
        "        \"فقد\", \"جراح\", \"نار\", \"هلاك\", \"ظلم\", \"ألم\", \"ضياع\"\n",
        "    ]):\n",
        "        return \"Perceived Severity (EPPM-1)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"قد يصيب\", \"معرض\", \"يصيب\", \"مرض\", \"أثر\", \"يصابون\", \"خوف\", \"قلق\",\n",
        "        \"ضعف\", \"خطر داهم\", \"يتعرض\", \"تهديدات\", \"فقدان\", \"يهدد\", \"مهدد\",\n",
        "        \"انهيار\", \"خطر محدق\", \"قريب\", \"ضحية\",\"توتر\"\n",
        "    ]):\n",
        "        return \"Perceived Susceptibility (EPPM-2)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"قادر\", \"يمكن\", \"يستطيع\", \"نستطيع\", \"حل\", \"امكانية\", \"القدرة\", \"التصدي\",\n",
        "        \"نصمد\", \"ننتصر\", \"نحمي\", \"نقف\", \"ثبات\", \"عزم\", \"تصدي\", \"مواجهة\",\n",
        "        \"تحقيق\", \"صمود\", \"إصرار\", \"نتقدم\", \"نجاح\"\n",
        "    ]):\n",
        "        return \"Perceived Self-Efficacy (EPPM-3)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"فعالة\", \"فعالية\", \"نجاح\", \"نتيجة\", \"الاستجابة\", \"أفضل\", \"مناسب\",\n",
        "        \"تحرير\", \"انتصار\", \"جدوى\", \"رد قوي\", \"تأثير\", \"حلول ناجحة\", \"نتائج إيجابية\",\n",
        "        \"أمل\", \"فائدة\", \"تحسن\", \"استجابة فعالة\", \"نصر\", \"بذل الجهود\", \"ثمار\",\"فوز\"\n",
        "    ]):\n",
        "        return \"Perceived Response-Efficacy (EPPM-4)\"\n",
        "\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['EPPM Category'] = df['Post Content'].apply(classify_eppm)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset4.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Classified EPPM posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdZzzgQvDGQO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtMAwiINDGH1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9NR1tnbAC_6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "file_path = \"comments.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "text_column = \"Comment Text\" if \"Comment Text\" in df.columns else df.columns[0]\n",
        "\n",
        "narrative_keywords = {\n",
        "    \"Engage\": [\"يا مغيث\", \"نداء\", \"الغضب\", \"التفاعل\", \"معركة\",\"يا\",\"يرتبط\",\"ارتباط\",\"مرتبط\",\"يتعلق\",\"تعليق\"],\n",
        "    \"Explain\": [\"معاني\", \"سبب\", \"شرح\", \"تفاصيل\", \"أسباب\", \"توضيح\", \"معنى\",\"هو\"],\n",
        "    \"Excite\": [\"حماس\", \"فرح\", \"مجد\", \"نصر\", \"افتخار\",\"فوز\",\"ربح\"],\n",
        "    \"Enhance\": [\"دعم\", \"تشجيع\", \"تعزيز\", \"وعي\", \"توعية\", \"تحفيز\",\"زيادة\",\"تزويد\"],\n",
        "    \"Dismiss\": [\"مبالغ\", \"تضخيم\", \"تافه\", \"ليس مهم\", \"وهم\", \"فقاعة\", \"تفاهة\",\"مستحيل\"],\n",
        "    \"Distort\": [\"تحريف\", \"تشويه\", \"تضليل\", \"كذب\", \"خداع\",\"مش ممكن\"],\n",
        "    \"Dismay\": [\"قلق\", \"حزن\", \"رعب\", \"ألم\", \"دموع\",\"الم\",\"خسارة\",\"فقد\"],\n",
        "    \"Distract\": [\"غير ذي صلة\", \"تحويل الأنظار\", \"موضوع آخر\", \"بعيد عن\", \"لا يرتبط\"]\n",
        "}\n",
        "\n",
        "def normalized_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "def classify_narrative(content):\n",
        "    if pd.isna(content) or not isinstance(content, str) or content.strip() == \"\":\n",
        "        return None\n",
        "    content = normalized_text(content)\n",
        "    for label, keywords in narrative_keywords.items():\n",
        "        if any(keyword in content for keyword in keywords):\n",
        "            return label\n",
        "    return \"Natural\"\n",
        "\n",
        "df[\"Narrative Manipulation (Comment)\"] = df[text_column].apply(classify_narrative)\n",
        "\n",
        "output_path = \"/content/comment/narrative_manipulation_comment.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Narrative manipulation analysis complete. Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BFKZmHyEDTs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/comment/narrative_manipulation_comment.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "against_keywords = [\n",
        "    \"الغاشم\", \"لا يمكن\", \"عنيف\", \"العدوان\",\n",
        "    \"تحذير\", \"خطر\", \"مشكلة\", \"انتقاد\", \"سلبي\", \"سقوط\",\"سلبي\",\"تقليل\",\"تخفيض\"\n",
        "    \"هجوم\", \"معارضة\", \"احتجاج\", \"تهديد\", \"تحريض\", \"معاداة\", \"خسارة\"\n",
        "]\n",
        "\n",
        "favor_keywords = [\n",
        "    \"انتصار\", \"دعم\", \"نجاح\", \"إشادة\", \"موافقة\", \"احتفال\", \"تأييد\",\"تزويد\",\"سرعة\",\n",
        "    \"مساندة\", \"حفل\", \"فرح\", \"نشر\", \"تمكين\", \"معقول\", \"فوز\", \"خدمة\", \"يخدم\", \"يدعم\",\"ايجابي\",\"إيجابي\",\"وووو\",\"يييي\"\n",
        "]\n",
        "\n",
        "def label_post(row):\n",
        "    content = str(row['Comment Text'])\n",
        "    content_lower = content.lower() if content != 'nan' else \"\"\n",
        "\n",
        "    if any(keyword in content_lower for keyword in against_keywords):\n",
        "            return \"Against\"\n",
        "    elif any(keyword in content_lower for keyword in favor_keywords):\n",
        "            return \"Favor\"\n",
        "\n",
        "    return \"Natural\"\n",
        "\n",
        "df['Stance (comment)'] = df.apply(label_post, axis=1)\n",
        "\n",
        "output_path = \"/content/comment/labeled_narrative_manipulation_posts.xlsx\"\n",
        "df.to_excel(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9_gwsjoFGHk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "file_path = \"/content/comment/labeled_narrative_manipulation_posts.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "post_ids = df['Comment ID']\n",
        "post_contents = df['Comment Text'].fillna('').str.lower()\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(post_contents)\n",
        "\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "similar_posts = []\n",
        "for i in range(cosine_sim_matrix.shape[0]):\n",
        "    for j in range(i + 1, cosine_sim_matrix.shape[1]):\n",
        "        if cosine_sim_matrix[i, j] >= similarity_threshold:\n",
        "            similar_posts.append({\n",
        "                'Comment ID 1': post_ids[i],\n",
        "                'Comment ID 2': post_ids[j],\n",
        "                'Similarity Score': cosine_sim_matrix[i, j],\n",
        "                'Comment1 Text': post_contents[i],\n",
        "                'Comment2 Text': post_contents[j]\n",
        "            })\n",
        "\n",
        "similar_posts_df = pd.DataFrame(similar_posts)\n",
        "\n",
        "if not similar_posts_df.empty:\n",
        "    output_file_path = '/content/comment/similar_posts_results_full_content.xlsx'\n",
        "    similar_posts_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"Results saved to {output_file_path}\")\n",
        "else:\n",
        "    print(\"No highly similar posts detected based on the chosen threshold.\")\n",
        "if similar_posts:\n",
        "    similarity_scores = [entry['Similarity Score'] for entry in similar_posts]\n",
        "    min_similarity = min(similarity_scores)\n",
        "    max_similarity = max(similarity_scores)\n",
        "    avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "    print(f\"Min: {min_similarity:.2f}, Max: {max_similarity:.2f}, Avg: {avg_similarity:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVpD9kA6FVsq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "main_file_path = \"/content/comment/labeled_narrative_manipulation_posts.xlsx\"\n",
        "df_main = pd.read_excel(main_file_path, sheet_name='Sheet1')\n",
        "\n",
        "similarity_file_path = '/content/comment/similar_posts_results_full_content.xlsx'\n",
        "df_similarity = pd.read_excel(similarity_file_path)\n",
        "\n",
        "most_similar_dict = {}\n",
        "\n",
        "for _, row in df_similarity.iterrows():\n",
        "    post1, post2, sim_score = row['Comment ID 1'], row['Comment ID 2'], row['Similarity Score']\n",
        "\n",
        "    if post1 not in most_similar_dict or sim_score > most_similar_dict[post1][1]:\n",
        "        most_similar_dict[post1] = (post2, sim_score)\n",
        "\n",
        "    if post2 not in most_similar_dict or sim_score > most_similar_dict[post2][1]:\n",
        "        most_similar_dict[post2] = (post1, sim_score)\n",
        "\n",
        "df_main['Most Similar Comment ID'] = df_main['Comment ID'].map(lambda x: most_similar_dict.get(x, (None, None))[0])\n",
        "df_main['Highest Similarity Score'] = df_main['Comment ID'].map(lambda x: most_similar_dict.get(x, (None, None))[1])\n",
        "\n",
        "df_main['Most Similar Comment ID'].fillna('No Match', inplace=True)\n",
        "df_main['Highest Similarity Score'].fillna(0, inplace=True)\n",
        "\n",
        "output_file = '/content/comment/updated_post_narrative_with_similarity.xlsx'\n",
        "df_main.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7km7F-0EF0WV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/comment/updated_post_narrative_with_similarity.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def elaboration_likelihood_model(content):\n",
        "    if pd.isna(content):\n",
        "        return \"Other\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"تصريحات\", \"التصريحات\", \"تصريح\", \"بيان\", \"أخبار\", \"التصريح\", \"معلومات\", \"الرسالة\", \"التفاصيل\", \"الحق\", \"يشرح\", \"كيف\", \"ما معنى\"]):\n",
        "        return \"Information\"\n",
        "    elif any(keyword in content for keyword in [\"?\", \"هل\", \"ماذا لو\", \"??\", \"؟؟\", \"؟\", \"!\", \"!!\"]):\n",
        "        return \"Question\"\n",
        "    elif any(keyword in content for keyword in [\"دعم\", \"تتبرع\", \"مظاهرة\", \"شارك\", \"بمشاركة\", \"تبرع\", \"والتبرع\", \"المشاركة\", \"دعما\", \"التبرع\", \"لتبرع\", \"مشاركته\", \"بتبرع\", \"التبرعات\", \"للمشاركة\", \"ودعما\", \"بالتبرع\", \"الدعم\", \"يدعم\", \"يشارك\", \"يساند\", \"اسناد\", \"إسناد\", \"دعماً\", \"مشاركة\"]):\n",
        "        return \"Participation\"\n",
        "    elif any(keyword in content for keyword in [\"زعيم\", \"لقائد\", \"سياسي\", \"قائد\", \"قائدنا\", \"القائد\", \"قادة\", \"القادة\", \"للقائد\", \"السياسي\", \"السياسية\", \"مشاهير\", \"الرئيس\", \"السنوار\", \"ابو عبيدة\", \"حزب الله\", \"القسام\", \"حماس\", \"نصرالله\"]):\n",
        "        return \"Celebrity\"\n",
        "    elif any(keyword in content for keyword in [\"نصرة\", \"نصرالله\", \"دعاء\", \"نصر\", \"إلهام\", \"النصر\", \"أمل\", \"اماني\", \"أماني\", \"المفاجأة\"]):\n",
        "        return \"Inspiration\"\n",
        "    elif any(keyword in content for keyword in [\"ضحك\", \"مزاح\", \"سخرية\", \"المضحك\", \"للسخرية\", \"بتمسخرو\", \"ههههههه\", \"ههههه\", \"ha\", \"هههههههههههه\", \"ضحكتني\", \"مسخرة\", \"ههه\", \"هههههههههههههههه\"]):\n",
        "        return \"Humor/Sarcasm\"\n",
        "    elif any(keyword in content for keyword in [\"ومشاهد\", \"شهادة\", \"الشهادة\", \"المشاهد\", \"رواية\", \"شاهد\", \"مشاهد\", \"قصة\", \"مشهد\", \"الحلم\", \"رواية\"]):\n",
        "        return \"Anecdotal/Story\"\n",
        "    else:\n",
        "        return \"Information\"\n",
        "\n",
        "df['elaboration_likelihood (Comment)'] = df['Comment Text'].apply(elaboration_likelihood_model)\n",
        "\n",
        "output_file = '/content/comment/categorized_posts.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJB0MKaHGBnr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset4.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def social_judgment_theory(content):\n",
        "    if pd.isna(content):\n",
        "        return \"\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"دواء\", \"صحة\", \"مرض\", \"شفاء\", \"علاج\",\"طب\",\"تمريض\",\"اصابة\"]):\n",
        "        return \"Health/Evidence-based Health Information\"\n",
        "    elif any(keyword in content for keyword in [\"النبي\", \"القرآن\", \"الإسلام\", \"دين\", \"صلاة\", \"الدين\", \"اليهودية\",\n",
        "                                                \"الجنة\", \"اليهود\", \"مسلمين\", \"مسلمون\", \"إسلام\",\"المسيحية\"]):\n",
        "        return \"Religion\"\n",
        "    elif any(keyword in content for keyword in [\"اختيار\", \"قرار\", \"حرية\", \"نظام\", \"حقوق\", \"رفاهية\",\"امكانية\",\"خيار\"]):\n",
        "        return \"Choice\"\n",
        "    elif any(keyword in content for keyword in [\"حكومة\", \"سياسي\", \"احتلال\", \"دولة\", \"جيش\", \"ثورة\", \"الدولة\",\n",
        "                                                \"سياسة\", \"السياسة\", \"المعتقلين\",\"ترامب\",\"بايدن\",\"دولي\"]):\n",
        "        return \"Political\"\n",
        "    elif any(keyword in content for keyword in [\"الله\",\"تضحية\", \"إيثار\", \"كرم\", \"نصر\", \"دعم\"]):\n",
        "        return \"Altruism\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['social_judgment (comment)'] = df['Comment Text'].apply(social_judgment_theory)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset5.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8t5Xu5oHGgC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset5.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def classify_eppm(content):\n",
        "    if pd.isna(content):\n",
        "        return \"\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\n",
        "        \"خطر\", \"كارثة\", \"تهديد\", \"خسارة\", \"دمار\", \"مأساة\", \"موت\", \"وفيات\",\n",
        "        \"قصف\", \"دماء\", \"حزن\", \"عذاب\", \"معاناة\", \"كارثة\", \"مجزرة\", \"خراب\",\n",
        "        \"فقد\", \"جراح\", \"نار\", \"هلاك\", \"ظلم\", \"ألم\", \"ضياع\"\n",
        "    ]):\n",
        "        return \"Perceived Severity (EPPM-1)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"قد يصيب\", \"معرض\", \"يصيب\", \"مرض\", \"أثر\", \"يصابون\", \"خوف\", \"قلق\",\n",
        "        \"ضعف\", \"خطر داهم\", \"يتعرض\", \"تهديدات\", \"فقدان\", \"يهدد\", \"مهدد\",\n",
        "        \"انهيار\", \"خطر محدق\", \"قريب\", \"ضحية\",\"توتر\"\n",
        "    ]):\n",
        "        return \"Perceived Susceptibility (EPPM-2)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"قادر\", \"يمكن\", \"يستطيع\", \"نستطيع\", \"حل\", \"امكانية\", \"القدرة\", \"التصدي\",\n",
        "        \"نصمد\", \"ننتصر\", \"نحمي\", \"نقف\", \"ثبات\", \"عزم\", \"تصدي\", \"مواجهة\",\n",
        "        \"تحقيق\", \"صمود\", \"إصرار\", \"نتقدم\", \"نجاح\"\n",
        "    ]):\n",
        "        return \"Perceived Self-Efficacy (EPPM-3)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"فعالة\", \"فعالية\", \"نجاح\", \"نتيجة\", \"الاستجابة\", \"أفضل\", \"مناسب\",\n",
        "        \"تحرير\", \"انتصار\", \"جدوى\", \"رد قوي\", \"تأثير\", \"حلول ناجحة\", \"نتائج إيجابية\",\n",
        "        \"أمل\", \"فائدة\", \"تحسن\", \"استجابة فعالة\", \"نصر\", \"بذل الجهود\", \"ثمار\",\"فوز\"\n",
        "    ]):\n",
        "        return \"Perceived Response-Efficacy (EPPM-4)\"\n",
        "\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['EPPM Category (Comment)'] = df['Comment Text'].apply(classify_eppm)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset6.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Classified EPPM posts saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMzVRyHjHhSe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "posts_file = 'posts_after_edits.xlsx'\n",
        "comments_file = 'comments_after_edits.xlsx'\n",
        "\n",
        "posts_df = pd.read_excel(posts_file)\n",
        "comments_df = pd.read_excel(comments_file)\n",
        "\n",
        "merged_df = posts_df.merge(comments_df, on='Post ID', how='left')\n",
        "\n",
        "merged_file = 'semi_final_Stage4_dataset.xlsx'\n",
        "merged_df.to_excel(merged_file, index=False)\n",
        "\n",
        "print(f\"Merged dataset saved to: {merged_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeRRsfQKiUHP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "post_columns = ['Post ID', 'Post Content', 'Post Time', 'Views', 'Forwards', 'Fire (Post)',\n",
        "                'Crying Face', 'Pray (Post)', 'Trophy (Post)', 'Clapping (Post)', 'Party (Post)',\n",
        "                'With Heart', 'Thumbs Up', 'Heart (Post)', 'Word Count', 'Words per Manip',\n",
        "                'Label', 'elaboration_likelihood', 'EPPM Category']\n",
        "\n",
        "df.loc[df.duplicated(subset=['Post ID'], keep='first'), post_columns] = ''\n",
        "\n",
        "output_file = 'cleaned_full_dataset.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrWdMfiwjyjN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "post_columns = ['Post ID', 'Post Content', 'Post Time', 'Views', 'Forwards', 'Fire (Post)',\n",
        "                'Crying Face', 'Pray (Post)', 'Trophy (Post)', 'Clapping (Post)', 'Party (Post)',\n",
        "                'With Heart', 'Thumbs Up', 'Heart (Post)', 'Word Count', 'Words per Manip',\n",
        "                'Label', 'ation_like_label', 'PM Categ']\n",
        "\n",
        "comment_counts = df.groupby('Post Content').size().reset_index(name='Comment Count')\n",
        "\n",
        "df = df.merge(comment_counts, on='Post Content', how='left')\n",
        "\n",
        "df.loc[df.duplicated(subset=['Post Content'], keep='first'), post_columns] = ''\n",
        "\n",
        "output_file = 'cleaned_dataset.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpuS7xN-bEec"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'new_full_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "columns_to_extract = ['Post Content', 'Post ID']\n",
        "df_extracted = df[columns_to_extract]\n",
        "\n",
        "output_path = 'extracted_posts.xlsx'\n",
        "df_extracted.to_excel(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqg49FUEv1bm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHlKUgGxyvZY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "file_path = \"extracted_posts.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "df = df[['Post Content']].dropna().reset_index(drop=True)\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    text = re.sub(r'[إأآا]', 'ا', text)\n",
        "    text = re.sub(r'ة', 'ه', text)\n",
        "    text = re.sub(r'ي', 'ى', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned Post\"] = df[\"Post Content\"].apply(clean_arabic_text)\n",
        "\n",
        "def label_post(text):\n",
        "    central_keywords = [\"دراسة\", \"بحث\", \"إحصائية\", \"بيانات\", \"إحصاء\", \"لأن\", \"بسبب\", \"إذا\", \"بحسب\", \"وفقًا ل\"]\n",
        "\n",
        "    peripheral_keywords = [\"كارثي\", \"مخيف\", \"خطير\", \"مصيبة\", \"مرعب\", \"يقول الخبراء\", \"إما معنا أو ضدنا\", \"تهديد وجودي\"]\n",
        "\n",
        "    if any(word in text for word in central_keywords):\n",
        "        return \"Central Cue\"\n",
        "    elif any(word in text for word in peripheral_keywords):\n",
        "        return \"Peripheral Cue\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "df[\"Label\"] = df[\"Cleaned Post\"].apply(label_post)\n",
        "\n",
        "df = df[df[\"Label\"] != \"Neutral\"].reset_index(drop=True)\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"Cleaned Post\"])\n",
        "y = df[\"Label\"].map({\"Central Cue\": 0, \"Peripheral Cue\": 1})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "df[\"Predicted Label\"] = model.predict(vectorizer.transform(df[\"Cleaned Post\"]))\n",
        "df[\"Predicted Label\"] = df[\"Predicted Label\"].map({0: \"Central Cue\", 1: \"Peripheral Cue\"})\n",
        "\n",
        "df.to_excel(\"classified_posts.xlsx\", index=False)\n",
        "print(\"Classification Completed! Results saved to classified_posts.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWcH_nYe2UiX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
