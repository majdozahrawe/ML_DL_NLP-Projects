{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-_AjjKEyCpS"
      },
      "outputs": [],
      "source": [
        "!pip install telethon\n",
        "!pip install nest_asyncio\n",
        "!pip install openpyxl\n",
        "!pip install telethon nest_asyncio openpyxl\n",
        "!pip install telethon nest_asyncio openpyxl nltk transformers sentence-transformers\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RqlaIcnh-WB"
      },
      "source": [
        "Posts and Comments Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AhCPKSy2ZnU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "from telethon import TelegramClient\n",
        "from telethon.errors import MsgIdInvalidError\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "nest_asyncio.apply()\n",
        "get them from telegram.\n",
        "api_id = 28524658\n",
        "api_hash = '5117388049357ea47ab79365e82750ae'\n",
        "create session to save login informations.\n",
        "client = TelegramClient('YD_Cryptocurrency', api_id, api_hash)\n",
        "Here define the date scope of posts\n",
        "start_date = datetime(2023, 1, 15, 1, 9, 14, tzinfo=timezone.utc)\n",
        "end_date = datetime(2025, 2, 17, 12, 8, 7, tzinfo=timezone.utc)\n",
        "Function to fetch and count post emoji reactions\n",
        "\n",
        "def count_post_reactions(reactions):\n",
        "    counts = {\n",
        "        'Heart': 0, 'Thumbs Up': 0, 'Thumbs Down': 0,\n",
        "        'Clapping Hands': 0, 'Exploding Head': 0, 'Broken Heart': 0\n",
        "    }\n",
        "\n",
        "    emoji_map = {\n",
        "        'â¤': 'Heart', 'ğŸ‘': 'Thumbs Up', 'ğŸ‘': 'Thumbs Down',\n",
        "        'ğŸ‘': 'Clapping Hands', 'ğŸ¤¯': 'Exploding Head', 'ğŸ’”': 'Broken Heart'\n",
        "    }\n",
        "\n",
        "    if reactions:\n",
        "        for reaction in reactions.results:\n",
        "            emoji = getattr(reaction.reaction, 'emoticon', None) or getattr(reaction.reaction, 'emoji', None)\n",
        "            if emoji in emoji_map:\n",
        "                counts[emoji_map[emoji]] += reaction.count\n",
        "\n",
        "    return counts\n",
        "\n",
        "function to fetch and count comment emoji reactions.\n",
        "\n",
        "def count_comment_reactions(reactions):\n",
        "    counts = {\n",
        "        'Heart': 0, 'Thumbs Up': 0\n",
        "    }\n",
        "\n",
        "    emoji_map = {\n",
        "        'â¤': 'Heart', 'ğŸ‘': 'Thumbs Up'\n",
        "    }\n",
        "    if reactions:\n",
        "        for reaction in reactions.results:\n",
        "            emoji = getattr(reaction.reaction, 'emoticon', None) or \\\n",
        "                    getattr(reaction.reaction, 'emoji', None)\n",
        "            if emoji == 'â¤':\n",
        "                counts['Heart'] += reaction.count\n",
        "            # elif emoji == 'ğŸ™':\n",
        "            #     counts['Pray'] += reaction.count\n",
        "            elif emoji == 'ğŸ‘':\n",
        "                counts['Thumbs Up'] += reaction.count\n",
        "            # elif emoji == 'ğŸ¥°':\n",
        "            #     counts['Face with Hearts'] += reaction.count\n",
        "\n",
        "    return counts\n",
        "\n",
        "this function used when the post has no comments.\n",
        "Function to fetch all data from the Telegram channel\n",
        "this main function used when work on colab or jupyter notebook to run the client and fetch data\n",
        "async def fetch_comments_for_posts(posts, channel):\n",
        "    tasks = [client.get_messages(channel, reply_to=post.id, limit=100) for post in posts]\n",
        "    return await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "async def fetch_data_with_comments(channel_username, batch_size=50, max_messages=400):\n",
        "    channel = await client.get_entity(channel_username)\n",
        "    posts = await fetch_data_from_channel(channel, batch_size=batch_size, max_messages=max_messages)\n",
        "\n",
        "    print(\"Fetching comments for all posts...\")\n",
        "    comments_data = await fetch_comments_for_posts(posts, channel)\n",
        "\n",
        "    structured_data = []\n",
        "    for post, comments in zip(posts, comments_data):\n",
        "        if post.text:\n",
        "            post_content = post.text.strip('**')\n",
        "            word_count = len(post_content.split())\n",
        "            avg_words_per_sentence = word_count / max(1, post_content.count('.') + post_content.count('!') + post_content.count('?'))\n",
        "            post_reactions = count_post_reactions(post.reactions)\n",
        "\n",
        "            comments_details = []\n",
        "            if isinstance(comments, list):\n",
        "                for comment in comments:\n",
        "                    if comment.text:\n",
        "                        comment_reactions = count_comment_reactions(comment.reactions)\n",
        "                        comments_details.append({\n",
        "                            'Comment ID': comment.id,\n",
        "                            'Comment Text': comment.text,\n",
        "                            'Comment Time': comment.date.isoformat(),\n",
        "                            **comment_reactions\n",
        "                        })\n",
        "\n",
        "            structured_data.append({\n",
        "                'Post ID': post.id,\n",
        "                'Post Content': post_content,\n",
        "                'Post Time': post.date.isoformat(),\n",
        "                'Views': post.views or 0,\n",
        "                'Forwards': post.forwards or 0,\n",
        "                **post_reactions,\n",
        "                'Word Count': word_count,\n",
        "                'Average Words per Sentence': avg_words_per_sentence,\n",
        "                'Comments Details': comments_details\n",
        "            })\n",
        "\n",
        "    print(f\"âœ… Total structured posts fetched: {len(structured_data)}\")\n",
        "    return structured_data\n",
        "\n",
        "def save_to_excel(data, filename='Ù†Ø§Ø´ÙŠÙˆÙ†Ø§Ù„ Ø¬ÙŠÙˆØºØ±Ø§ÙÙŠÚ¯.xlsx'):\n",
        "    rows = []\n",
        "    for post in data:\n",
        "        if 'Comments Details' not in post or not post['Comments Details']:\n",
        "            post['Comments Details'] = [{'Comment ID': None, 'Comment Text': None, 'Comment Time': None,\n",
        "                                         'Heart': 0,\n",
        "                                         'Thumbs Up': 0,}]\n",
        "\n",
        "        for comment in post['Comments Details']:\n",
        "            rows.append({\n",
        "                'Post ID': post['Post ID'],\n",
        "                'Post Content': post['Post Content'],\n",
        "                'Post Time': post['Post Time'],\n",
        "                'Views': post['Views'],\n",
        "                'Forwards': post['Forwards'],\n",
        "                **{key + ' (Post)': post.get(key, 0) for key in ['Heart', 'Thumbs Up', 'Thumbs Down', 'Clapping Hands', 'Exploding Head', 'Broken Heart']},\n",
        "                'Word Count': post['Word Count'],\n",
        "                'Average Words per Sentence': post['Average Words per Sentence'],\n",
        "                'Comment ID': comment['Comment ID'],\n",
        "                'Comment Text': comment['Comment Text'],\n",
        "                'Comment Time': comment['Comment Time'],\n",
        "                **{key + ' (Comment)': comment.get(key, 0) for key in ['Thumbs Up','Heart']}\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_excel(filename, index=False, engine='openpyxl')\n",
        "    print(f\"âœ… Data successfully saved to {filename}\")\n",
        "\n",
        "async def main():\n",
        "    async with client:\n",
        "        data = await fetch_data_with_comments('@asbwf')  # âœ… Fetch data\n",
        "        save_to_excel(data)  # âœ… Save as Excel (.xlsx)\n",
        "\n",
        "asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrvFFTtuzalg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "datasets = [\n",
        "    \"Youcef YD - Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠÙ‡ ğŸ†ğŸ….xlsx\",\n",
        "    \"Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª _ YSM Sports.xlsx\",\n",
        "    \"Ø¨Ø±Ù…Ø¬Ø© ÙˆØªØ·ÙˆÙŠØ±.xlsx\",\n",
        "    \"Ø¹ØµÙŠØ± Ø§Ù„ÙƒØªØ¨ - PDF.xlsx\",\n",
        "    \"Ù†Ø§Ø´ÙŠÙˆÙ†Ø§Ù„ Ø¬ÙŠÙˆØºØ±Ø§ÙÙŠÚ¯.xlsx\"\n",
        "]\n",
        "\n",
        "frames = [pd.read_excel(file) for file in datasets]\n",
        "\n",
        "merged_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "\n",
        "output_path = \"merged_dataset.xlsx\"\n",
        "merged_df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Merged dataset saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI1-_TOVy8vC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'modified_dataset.xlsx'\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "df = xls.parse(xls.sheet_names[0])\n",
        "\n",
        "post_columns = [\n",
        "    'Post ID', 'Post Content', 'Post Time', 'Views', 'Forwards', 'Fire (Post)',\n",
        "    'Saluting Face (Post)', 'Pray (Post)', 'Trophy (Post)', 'Clapping (Post)',\n",
        "    'Party (Post)', 'Face with Hearts (Post)', 'Thumbs Up (Post)', 'Heart (Post)',\n",
        "    'Word Count', 'Average Words per Sentence'\n",
        "]\n",
        "\n",
        "\n",
        "comment_columns = [\n",
        "  'Post ID','Comment ID', 'Comment Text', 'Comment Time', 'Heart (Comment)', 'Thumbs Up (Comment)'\n",
        "]\n",
        "\n",
        "posts_df = df[post_columns].drop_duplicates(subset=['Post ID'])\n",
        "comments_df = df[comment_columns].dropna(subset=['Comment ID'])\n",
        "\n",
        "posts_file = 'posts.xlsx'\n",
        "comments_file = 'commentsID.xlsx'\n",
        "\n",
        "comments_df.to_excel(comments_file, index=False)\n",
        "\n",
        "print(f\"Posts saved to: {posts_file}\")\n",
        "print(f\"Comments saved to: {comments_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI-pYJNQiDRv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "file_path = \"posts.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "text_column = \"Post Content\" if \"Post Content\" in df.columns else df.columns[0]\n",
        "\n",
        "narrative_keywords = {\n",
        "    \"Engage\": [\"ÙŠØ§ Ù…ØºÙŠØ«\", \"Ù†Ø¯Ø§Ø¡\", \"Ø§Ù„ØºØ¶Ø¨\", \"Ø§Ù„ØªÙØ§Ø¹Ù„\", \"Ù…Ø¹Ø±ÙƒØ©\"],\n",
        "    \"Explain\": [\"Ù…Ø¹Ø§Ù†ÙŠ\", \"Ø³Ø¨Ø¨\", \"Ø´Ø±Ø­\", \"ØªÙØ§ØµÙŠÙ„\", \"Ø£Ø³Ø¨Ø§Ø¨\", \"ØªÙˆØ¶ÙŠØ­\", \"Ù…Ø¹Ù†Ù‰\"],\n",
        "    \"Excite\": [\"Ø­Ù…Ø§Ø³\", \"ÙØ±Ø­\", \"Ù…Ø¬Ø¯\", \"Ù†ØµØ±\", \"Ø§ÙØªØ®Ø§Ø±\"],\n",
        "    \"Enhance\": [\"Ø¯Ø¹Ù…\", \"ØªØ´Ø¬ÙŠØ¹\", \"ØªØ¹Ø²ÙŠØ²\", \"ÙˆØ¹ÙŠ\", \"ØªÙˆØ¹ÙŠØ©\", \"ØªØ­ÙÙŠØ²\"],\n",
        "    \"Dismiss\": [\"Ù…Ø¨Ø§Ù„Øº\", \"ØªØ¶Ø®ÙŠÙ…\", \"ØªØ§ÙÙ‡\", \"Ù„ÙŠØ³ Ù…Ù‡Ù…\", \"ÙˆÙ‡Ù…\", \"ÙÙ‚Ø§Ø¹Ø©\", \"ØªÙØ§Ù‡Ø©\"],\n",
        "    \"Distort\": [\"ØªØ­Ø±ÙŠÙ\", \"ØªØ´ÙˆÙŠÙ‡\", \"ØªØ¶Ù„ÙŠÙ„\", \"ÙƒØ°Ø¨\", \"Ø®Ø¯Ø§Ø¹\"],\n",
        "    \"Dismay\": [\"Ù‚Ù„Ù‚\", \"Ø­Ø²Ù†\", \"Ø±Ø¹Ø¨\", \"Ø£Ù„Ù…\", \"Ø¯Ù…ÙˆØ¹\"],\n",
        "    \"Distract\": [\"ØºÙŠØ± Ø°ÙŠ ØµÙ„Ø©\", \"ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù†Ø¸Ø§Ø±\", \"Ù…ÙˆØ¶ÙˆØ¹ Ø¢Ø®Ø±\", \"Ø¨Ø¹ÙŠØ¯ Ø¹Ù†\", \"Ù„Ø§ ÙŠØ±ØªØ¨Ø·\"]\n",
        "}\n",
        "\n",
        "def normalized_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "def classify_narrative(content):\n",
        "    if pd.isna(content) or not isinstance(content, str) or content.strip() == \"\":\n",
        "        return None\n",
        "    content = normalized_text(content)\n",
        "    for label, keywords in narrative_keywords.items():\n",
        "        if any(keyword in content for keyword in keywords):\n",
        "            return label\n",
        "    return \"Natural\"\n",
        "\n",
        "df[\"Narrative Manipulation\"] = df[text_column].apply(classify_narrative)\n",
        "\n",
        "output_path = \"narrative_manipulation_posts.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Narrative manipulation analysis complete. Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwHEPEauMsR0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"narrative_manipulation_posts.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "against_keywords = [\n",
        "    \"Ø§Ù„ØºØ§Ø´Ù…\", \"Ù„Ø§ ÙŠÙ…ÙƒÙ†\", \"Ø¹Ù†ÙŠÙ\", \"Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†\",\n",
        "    \"ØªØ­Ø°ÙŠØ±\", \"Ø®Ø·Ø±\", \"Ù…Ø´ÙƒÙ„Ø©\", \"Ø§Ù†ØªÙ‚Ø§Ø¯\", \"Ø³Ù„Ø¨ÙŠ\", \"Ø³Ù‚ÙˆØ·\",\"Ø³Ù„Ø¨ÙŠ\",\"ØªÙ‚Ù„ÙŠÙ„\",\"ØªØ®ÙÙŠØ¶\"\n",
        "    \"Ù‡Ø¬ÙˆÙ…\", \"Ù…Ø¹Ø§Ø±Ø¶Ø©\", \"Ø§Ø­ØªØ¬Ø§Ø¬\", \"ØªÙ‡Ø¯ÙŠØ¯\", \"ØªØ­Ø±ÙŠØ¶\", \"Ù…Ø¹Ø§Ø¯Ø§Ø©\", \"Ø®Ø³Ø§Ø±Ø©\"\n",
        "]\n",
        "\n",
        "favor_keywords = [\n",
        "    \"Ø§Ù†ØªØµØ§Ø±\", \"Ø¯Ø¹Ù…\", \"Ù†Ø¬Ø§Ø­\", \"Ø¥Ø´Ø§Ø¯Ø©\", \"Ù…ÙˆØ§ÙÙ‚Ø©\", \"Ø§Ø­ØªÙØ§Ù„\", \"ØªØ£ÙŠÙŠØ¯\",\"ØªØ²ÙˆÙŠØ¯\",\"Ø³Ø±Ø¹Ø©\",\n",
        "    \"Ù…Ø³Ø§Ù†Ø¯Ø©\", \"Ø­ÙÙ„\", \"ÙØ±Ø­\", \"Ù†Ø´Ø±\", \"ØªÙ…ÙƒÙŠÙ†\", \"Ù…Ø¹Ù‚ÙˆÙ„\", \"ÙÙˆØ²\", \"Ø®Ø¯Ù…Ø©\", \"ÙŠØ®Ø¯Ù…\", \"ÙŠØ¯Ø¹Ù…\",\"Ø§ÙŠØ¬Ø§Ø¨ÙŠ\",\"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ\",\"ÙˆÙˆÙˆÙˆ\",\"ÙŠÙŠÙŠÙŠ\"\n",
        "]\n",
        "\n",
        "def label_post(row):\n",
        "    content = str(row['Post Content'])\n",
        "    content_lower = content.lower() if content != 'nan' else \"\"\n",
        "\n",
        "    # down_broken_reactions = row.get('Thumbs Down (Post)', 0) + row.get('Broken Heart (Post)', 0)\n",
        "    # heart_thumbs_reactions = row.get('Heart (Post)', 0) + row.get('Thumbs Up (Post)', 0)\n",
        "\n",
        "    if any(keyword in content_lower for keyword in against_keywords):\n",
        "            return \"Against\"\n",
        "    elif any(keyword in content_lower for keyword in favor_keywords):\n",
        "            return \"Favor\"\n",
        "\n",
        "    return \"Natural\"\n",
        "\n",
        "df['Label'] = df.apply(label_post, axis=1)\n",
        "\n",
        "output_path = \"labeled_narrative_manipulation_posts.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrg8l1yQt65e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "file_path = 'classified_eppm_posts.xlsx'\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "post_ids = df['Post ID']\n",
        "post_contents = df['Post Content'].fillna('').str.lower()\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(post_contents)\n",
        "\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "similar_posts = []\n",
        "for i in range(cosine_sim_matrix.shape[0]):\n",
        "    for j in range(i + 1, cosine_sim_matrix.shape[1]):\n",
        "        if cosine_sim_matrix[i, j] >= similarity_threshold:\n",
        "            similar_posts.append({\n",
        "                'Post ID 1': post_ids[i],\n",
        "                'Post ID 2': post_ids[j],\n",
        "                'Similarity Score': cosine_sim_matrix[i, j],\n",
        "                'Post1 Content': post_contents[i],\n",
        "                'Post2 Content': post_contents[j]\n",
        "            })\n",
        "\n",
        "similar_posts_df = pd.DataFrame(similar_posts)\n",
        "\n",
        "if not similar_posts_df.empty:\n",
        "    output_file_path = 'similar_posts_results_full_content.xlsx'\n",
        "    similar_posts_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"Results saved to {output_file_path}\")\n",
        "else:\n",
        "    print(\"No highly similar posts detected based on the chosen threshold.\")\n",
        "\n",
        "if similar_posts:\n",
        "    similarity_scores = [entry['Similarity Score'] for entry in similar_posts]\n",
        "    min_similarity = min(similarity_scores)\n",
        "    max_similarity = max(similarity_scores)\n",
        "    avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "    print(f\"Min: {min_similarity:.2f}, Max: {max_similarity:.2f}, Avg: {avg_similarity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffSck_0mBtA_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "main_file_path = 'classified_eppm_posts.xlsx'\n",
        "df_main = pd.read_excel(main_file_path, sheet_name='Sheet1')\n",
        "\n",
        "similarity_file_path = 'similar_posts_results_full_content.xlsx'\n",
        "df_similarity = pd.read_excel(similarity_file_path)\n",
        "\n",
        "most_similar_dict = {}\n",
        "\n",
        "for _, row in df_similarity.iterrows():\n",
        "    post1, post2, sim_score = row['Post ID 1'], row['Post ID 2'], row['Similarity Score']\n",
        "\n",
        "    if post1 not in most_similar_dict or sim_score > most_similar_dict[post1][1]:\n",
        "        most_similar_dict[post1] = (post2, sim_score)\n",
        "\n",
        "    if post2 not in most_similar_dict or sim_score > most_similar_dict[post2][1]:\n",
        "        most_similar_dict[post2] = (post1, sim_score)\n",
        "\n",
        "df_main['Most Similar Post ID'] = df_main['Post ID'].map(lambda x: most_similar_dict.get(x, (None, None))[0])\n",
        "df_main['Highest Similarity Score'] = df_main['Post ID'].map(lambda x: most_similar_dict.get(x, (None, None))[1])\n",
        "\n",
        "df_main['Most Similar Post ID'].fillna('No Match', inplace=True)\n",
        "df_main['Highest Similarity Score'].fillna(0, inplace=True)\n",
        "\n",
        "output_file = 'updated_post_narrative_with_similarity.xlsx'\n",
        "df_main.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8stP0cVV9KI_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'labeled_narrative_manipulation_posts.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def elaboration_likelihood_model(content):\n",
        "    if pd.isna(content):\n",
        "        return \"Other\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"ØªØµØ±ÙŠØ­Ø§Øª\", \"Ø§Ù„ØªØµØ±ÙŠØ­Ø§Øª\", \"ØªØµØ±ÙŠØ­\", \"Ø¨ÙŠØ§Ù†\", \"Ø£Ø®Ø¨Ø§Ø±\", \"Ø§Ù„ØªØµØ±ÙŠØ­\", \"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\", \"Ø§Ù„Ø±Ø³Ø§Ù„Ø©\", \"Ø§Ù„ØªÙØ§ØµÙŠÙ„\", \"Ø§Ù„Ø­Ù‚\", \"ÙŠØ´Ø±Ø­\", \"ÙƒÙŠÙ\", \"Ù…Ø§ Ù…Ø¹Ù†Ù‰\"]):\n",
        "        return \"Information\"\n",
        "    elif any(keyword in content for keyword in [\"?\", \"Ù‡Ù„\", \"Ù…Ø§Ø°Ø§ Ù„Ùˆ\", \"??\", \"ØŸØŸ\", \"ØŸ\", \"!\", \"!!\"]):\n",
        "        return \"Question\"\n",
        "    elif any(keyword in content for keyword in [\"Ø¯Ø¹Ù…\", \"ØªØªØ¨Ø±Ø¹\", \"Ù…Ø¸Ø§Ù‡Ø±Ø©\", \"Ø´Ø§Ø±Ùƒ\", \"Ø¨Ù…Ø´Ø§Ø±ÙƒØ©\", \"ØªØ¨Ø±Ø¹\", \"ÙˆØ§Ù„ØªØ¨Ø±Ø¹\", \"Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©\", \"Ø¯Ø¹Ù…Ø§\", \"Ø§Ù„ØªØ¨Ø±Ø¹\", \"Ù„ØªØ¨Ø±Ø¹\", \"Ù…Ø´Ø§Ø±ÙƒØªÙ‡\", \"Ø¨ØªØ¨Ø±Ø¹\", \"Ø§Ù„ØªØ¨Ø±Ø¹Ø§Øª\", \"Ù„Ù„Ù…Ø´Ø§Ø±ÙƒØ©\", \"ÙˆØ¯Ø¹Ù…Ø§\", \"Ø¨Ø§Ù„ØªØ¨Ø±Ø¹\", \"Ø§Ù„Ø¯Ø¹Ù…\", \"ÙŠØ¯Ø¹Ù…\", \"ÙŠØ´Ø§Ø±Ùƒ\", \"ÙŠØ³Ø§Ù†Ø¯\", \"Ø§Ø³Ù†Ø§Ø¯\", \"Ø¥Ø³Ù†Ø§Ø¯\", \"Ø¯Ø¹Ù…Ø§Ù‹\", \"Ù…Ø´Ø§Ø±ÙƒØ©\"]):\n",
        "        return \"Participation\"\n",
        "    elif any(keyword in content for keyword in [\"Ø²Ø¹ÙŠÙ…\", \"Ù„Ù‚Ø§Ø¦Ø¯\", \"Ø³ÙŠØ§Ø³ÙŠ\", \"Ù‚Ø§Ø¦Ø¯\", \"Ù‚Ø§Ø¦Ø¯Ù†Ø§\", \"Ø§Ù„Ù‚Ø§Ø¦Ø¯\", \"Ù‚Ø§Ø¯Ø©\", \"Ø§Ù„Ù‚Ø§Ø¯Ø©\", \"Ù„Ù„Ù‚Ø§Ø¦Ø¯\", \"Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ\", \"Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ©\", \"Ù…Ø´Ø§Ù‡ÙŠØ±\", \"Ø§Ù„Ø±Ø¦ÙŠØ³\", \"Ø§Ù„Ø³Ù†ÙˆØ§Ø±\", \"Ø§Ø¨Ùˆ Ø¹Ø¨ÙŠØ¯Ø©\", \"Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡\", \"Ø§Ù„Ù‚Ø³Ø§Ù…\", \"Ø­Ù…Ø§Ø³\", \"Ù†ØµØ±Ø§Ù„Ù„Ù‡\"]):\n",
        "        return \"Celebrity\"\n",
        "    elif any(keyword in content for keyword in [\"Ù†ØµØ±Ø©\", \"Ù†ØµØ±Ø§Ù„Ù„Ù‡\", \"Ø¯Ø¹Ø§Ø¡\", \"Ù†ØµØ±\", \"Ø¥Ù„Ù‡Ø§Ù…\", \"Ø§Ù„Ù†ØµØ±\", \"Ø£Ù…Ù„\", \"Ø§Ù…Ø§Ù†ÙŠ\", \"Ø£Ù…Ø§Ù†ÙŠ\", \"Ø§Ù„Ù…ÙØ§Ø¬Ø£Ø©\"]):\n",
        "        return \"Inspiration\"\n",
        "    elif any(keyword in content for keyword in [\"Ø¶Ø­Ùƒ\", \"Ù…Ø²Ø§Ø­\", \"Ø³Ø®Ø±ÙŠØ©\", \"Ø§Ù„Ù…Ø¶Ø­Ùƒ\", \"Ù„Ù„Ø³Ø®Ø±ÙŠØ©\", \"Ø¨ØªÙ…Ø³Ø®Ø±Ùˆ\", \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\", \"Ù‡Ù‡Ù‡Ù‡Ù‡\", \"ha\", \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\", \"Ø¶Ø­ÙƒØªÙ†ÙŠ\", \"Ù…Ø³Ø®Ø±Ø©\", \"Ù‡Ù‡Ù‡\", \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\"]):\n",
        "        return \"Humor/Sarcasm\"\n",
        "    elif any(keyword in content for keyword in [\"ÙˆÙ…Ø´Ø§Ù‡Ø¯\", \"Ø´Ù‡Ø§Ø¯Ø©\", \"Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©\", \"Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯\", \"Ø±ÙˆØ§ÙŠØ©\", \"Ø´Ø§Ù‡Ø¯\", \"Ù…Ø´Ø§Ù‡Ø¯\", \"Ù‚ØµØ©\", \"Ù…Ø´Ù‡Ø¯\", \"Ø§Ù„Ø­Ù„Ù…\", \"Ø±ÙˆØ§ÙŠØ©\"]):\n",
        "        return \"Anecdotal/Story\"\n",
        "    else:\n",
        "        return \"Information\"\n",
        "\n",
        "df['elaboration_likelihood'] = df['Post Content'].apply(elaboration_likelihood_model)\n",
        "\n",
        "output_file = 'categorized_posts.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "428QTcna-PG0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def social_judgment_theory(content):\n",
        "    if pd.isna(content):\n",
        "        return \"\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"Ø¯ÙˆØ§Ø¡\", \"ØµØ­Ø©\", \"Ù…Ø±Ø¶\", \"Ø´ÙØ§Ø¡\", \"Ø¹Ù„Ø§Ø¬\",\"Ø·Ø¨\",\"ØªÙ…Ø±ÙŠØ¶\",\"Ø§ØµØ§Ø¨Ø©\"]):\n",
        "        return \"Health/Evidence-based Health Information\"\n",
        "    elif any(keyword in content for keyword in [\"Ø§Ù„Ù†Ø¨ÙŠ\", \"Ø§Ù„Ù‚Ø±Ø¢Ù†\", \"Ø§Ù„Ø¥Ø³Ù„Ø§Ù…\", \"Ø¯ÙŠÙ†\", \"ØµÙ„Ø§Ø©\", \"Ø§Ù„Ø¯ÙŠÙ†\", \"Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ©\",\n",
        "                                                \"Ø§Ù„Ø¬Ù†Ø©\", \"Ø§Ù„ÙŠÙ‡ÙˆØ¯\", \"Ù…Ø³Ù„Ù…ÙŠÙ†\", \"Ù…Ø³Ù„Ù…ÙˆÙ†\", \"Ø¥Ø³Ù„Ø§Ù…\",\"Ø§Ù„Ù…Ø³ÙŠØ­ÙŠØ©\"]):\n",
        "        return \"Religion\"\n",
        "    elif any(keyword in content for keyword in [\"Ø§Ø®ØªÙŠØ§Ø±\", \"Ù‚Ø±Ø§Ø±\", \"Ø­Ø±ÙŠØ©\", \"Ù†Ø¸Ø§Ù…\", \"Ø­Ù‚ÙˆÙ‚\", \"Ø±ÙØ§Ù‡ÙŠØ©\",\"Ø§Ù…ÙƒØ§Ù†ÙŠØ©\",\"Ø®ÙŠØ§Ø±\"]):\n",
        "        return \"Choice\"\n",
        "    elif any(keyword in content for keyword in [\"Ø­ÙƒÙˆÙ…Ø©\", \"Ø³ÙŠØ§Ø³ÙŠ\", \"Ø§Ø­ØªÙ„Ø§Ù„\", \"Ø¯ÙˆÙ„Ø©\", \"Ø¬ÙŠØ´\", \"Ø«ÙˆØ±Ø©\", \"Ø§Ù„Ø¯ÙˆÙ„Ø©\",\n",
        "                                                \"Ø³ÙŠØ§Ø³Ø©\", \"Ø§Ù„Ø³ÙŠØ§Ø³Ø©\", \"Ø§Ù„Ù…Ø¹ØªÙ‚Ù„ÙŠÙ†\",\"ØªØ±Ø§Ù…Ø¨\",\"Ø¨Ø§ÙŠØ¯Ù†\",\"Ø¯ÙˆÙ„ÙŠ\"]):\n",
        "        return \"Political\"\n",
        "    elif any(keyword in content for keyword in [\"Ø§Ù„Ù„Ù‡\",\"ØªØ¶Ø­ÙŠØ©\", \"Ø¥ÙŠØ«Ø§Ø±\", \"ÙƒØ±Ù…\", \"Ù†ØµØ±\", \"Ø¯Ø¹Ù…\"]):\n",
        "        return \"Altruism\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['social_judgment'] = df['Post Content'].apply(social_judgment_theory)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset3.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnpD6DvD_u68"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset3.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def classify_eppm(content):\n",
        "    if pd.isna(content):\n",
        "        return \"Other\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\n",
        "        \"Ø®Ø·Ø±\", \"ÙƒØ§Ø±Ø«Ø©\", \"ØªÙ‡Ø¯ÙŠØ¯\", \"Ø®Ø³Ø§Ø±Ø©\", \"Ø¯Ù…Ø§Ø±\", \"Ù…Ø£Ø³Ø§Ø©\", \"Ù…ÙˆØª\", \"ÙˆÙÙŠØ§Øª\",\n",
        "        \"Ù‚ØµÙ\", \"Ø¯Ù…Ø§Ø¡\", \"Ø­Ø²Ù†\", \"Ø¹Ø°Ø§Ø¨\", \"Ù…Ø¹Ø§Ù†Ø§Ø©\", \"ÙƒØ§Ø±Ø«Ø©\", \"Ù…Ø¬Ø²Ø±Ø©\", \"Ø®Ø±Ø§Ø¨\",\n",
        "        \"ÙÙ‚Ø¯\", \"Ø¬Ø±Ø§Ø­\", \"Ù†Ø§Ø±\", \"Ù‡Ù„Ø§Ùƒ\", \"Ø¸Ù„Ù…\", \"Ø£Ù„Ù…\", \"Ø¶ÙŠØ§Ø¹\"\n",
        "    ]):\n",
        "        return \"Perceived Severity (EPPM-1)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"Ù‚Ø¯ ÙŠØµÙŠØ¨\", \"Ù…Ø¹Ø±Ø¶\", \"ÙŠØµÙŠØ¨\", \"Ù…Ø±Ø¶\", \"Ø£Ø«Ø±\", \"ÙŠØµØ§Ø¨ÙˆÙ†\", \"Ø®ÙˆÙ\", \"Ù‚Ù„Ù‚\",\n",
        "        \"Ø¶Ø¹Ù\", \"Ø®Ø·Ø± Ø¯Ø§Ù‡Ù…\", \"ÙŠØªØ¹Ø±Ø¶\", \"ØªÙ‡Ø¯ÙŠØ¯Ø§Øª\", \"ÙÙ‚Ø¯Ø§Ù†\", \"ÙŠÙ‡Ø¯Ø¯\", \"Ù…Ù‡Ø¯Ø¯\",\n",
        "        \"Ø§Ù†Ù‡ÙŠØ§Ø±\", \"Ø®Ø·Ø± Ù…Ø­Ø¯Ù‚\", \"Ù‚Ø±ÙŠØ¨\", \"Ø¶Ø­ÙŠØ©\",\"ØªÙˆØªØ±\"\n",
        "    ]):\n",
        "        return \"Perceived Susceptibility (EPPM-2)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"Ù‚Ø§Ø¯Ø±\", \"ÙŠÙ…ÙƒÙ†\", \"ÙŠØ³ØªØ·ÙŠØ¹\", \"Ù†Ø³ØªØ·ÙŠØ¹\", \"Ø­Ù„\", \"Ø§Ù…ÙƒØ§Ù†ÙŠØ©\", \"Ø§Ù„Ù‚Ø¯Ø±Ø©\", \"Ø§Ù„ØªØµØ¯ÙŠ\",\n",
        "        \"Ù†ØµÙ…Ø¯\", \"Ù†Ù†ØªØµØ±\", \"Ù†Ø­Ù…ÙŠ\", \"Ù†Ù‚Ù\", \"Ø«Ø¨Ø§Øª\", \"Ø¹Ø²Ù…\", \"ØªØµØ¯ÙŠ\", \"Ù…ÙˆØ§Ø¬Ù‡Ø©\",\n",
        "        \"ØªØ­Ù‚ÙŠÙ‚\", \"ØµÙ…ÙˆØ¯\", \"Ø¥ØµØ±Ø§Ø±\", \"Ù†ØªÙ‚Ø¯Ù…\", \"Ù†Ø¬Ø§Ø­\"\n",
        "    ]):\n",
        "        return \"Perceived Self-Efficacy (EPPM-3)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"ÙØ¹Ø§Ù„Ø©\", \"ÙØ¹Ø§Ù„ÙŠØ©\", \"Ù†Ø¬Ø§Ø­\", \"Ù†ØªÙŠØ¬Ø©\", \"Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©\", \"Ø£ÙØ¶Ù„\", \"Ù…Ù†Ø§Ø³Ø¨\",\n",
        "        \"ØªØ­Ø±ÙŠØ±\", \"Ø§Ù†ØªØµØ§Ø±\", \"Ø¬Ø¯ÙˆÙ‰\", \"Ø±Ø¯ Ù‚ÙˆÙŠ\", \"ØªØ£Ø«ÙŠØ±\", \"Ø­Ù„ÙˆÙ„ Ù†Ø§Ø¬Ø­Ø©\", \"Ù†ØªØ§Ø¦Ø¬ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©\",\n",
        "        \"Ø£Ù…Ù„\", \"ÙØ§Ø¦Ø¯Ø©\", \"ØªØ­Ø³Ù†\", \"Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙØ¹Ø§Ù„Ø©\", \"Ù†ØµØ±\", \"Ø¨Ø°Ù„ Ø§Ù„Ø¬Ù‡ÙˆØ¯\", \"Ø«Ù…Ø§Ø±\",\"ÙÙˆØ²\"\n",
        "    ]):\n",
        "        return \"Perceived Response-Efficacy (EPPM-4)\"\n",
        "\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['EPPM Category'] = df['Post Content'].apply(classify_eppm)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset4.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Classified EPPM posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdZzzgQvDGQO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtMAwiINDGH1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9NR1tnbAC_6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "file_path = \"comments.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "text_column = \"Comment Text\" if \"Comment Text\" in df.columns else df.columns[0]\n",
        "\n",
        "narrative_keywords = {\n",
        "    \"Engage\": [\"ÙŠØ§ Ù…ØºÙŠØ«\", \"Ù†Ø¯Ø§Ø¡\", \"Ø§Ù„ØºØ¶Ø¨\", \"Ø§Ù„ØªÙØ§Ø¹Ù„\", \"Ù…Ø¹Ø±ÙƒØ©\",\"ÙŠØ§\",\"ÙŠØ±ØªØ¨Ø·\",\"Ø§Ø±ØªØ¨Ø§Ø·\",\"Ù…Ø±ØªØ¨Ø·\",\"ÙŠØªØ¹Ù„Ù‚\",\"ØªØ¹Ù„ÙŠÙ‚\"],\n",
        "    \"Explain\": [\"Ù…Ø¹Ø§Ù†ÙŠ\", \"Ø³Ø¨Ø¨\", \"Ø´Ø±Ø­\", \"ØªÙØ§ØµÙŠÙ„\", \"Ø£Ø³Ø¨Ø§Ø¨\", \"ØªÙˆØ¶ÙŠØ­\", \"Ù…Ø¹Ù†Ù‰\",\"Ù‡Ùˆ\"],\n",
        "    \"Excite\": [\"Ø­Ù…Ø§Ø³\", \"ÙØ±Ø­\", \"Ù…Ø¬Ø¯\", \"Ù†ØµØ±\", \"Ø§ÙØªØ®Ø§Ø±\",\"ÙÙˆØ²\",\"Ø±Ø¨Ø­\"],\n",
        "    \"Enhance\": [\"Ø¯Ø¹Ù…\", \"ØªØ´Ø¬ÙŠØ¹\", \"ØªØ¹Ø²ÙŠØ²\", \"ÙˆØ¹ÙŠ\", \"ØªÙˆØ¹ÙŠØ©\", \"ØªØ­ÙÙŠØ²\",\"Ø²ÙŠØ§Ø¯Ø©\",\"ØªØ²ÙˆÙŠØ¯\"],\n",
        "    \"Dismiss\": [\"Ù…Ø¨Ø§Ù„Øº\", \"ØªØ¶Ø®ÙŠÙ…\", \"ØªØ§ÙÙ‡\", \"Ù„ÙŠØ³ Ù…Ù‡Ù…\", \"ÙˆÙ‡Ù…\", \"ÙÙ‚Ø§Ø¹Ø©\", \"ØªÙØ§Ù‡Ø©\",\"Ù…Ø³ØªØ­ÙŠÙ„\"],\n",
        "    \"Distort\": [\"ØªØ­Ø±ÙŠÙ\", \"ØªØ´ÙˆÙŠÙ‡\", \"ØªØ¶Ù„ÙŠÙ„\", \"ÙƒØ°Ø¨\", \"Ø®Ø¯Ø§Ø¹\",\"Ù…Ø´ Ù…Ù…ÙƒÙ†\"],\n",
        "    \"Dismay\": [\"Ù‚Ù„Ù‚\", \"Ø­Ø²Ù†\", \"Ø±Ø¹Ø¨\", \"Ø£Ù„Ù…\", \"Ø¯Ù…ÙˆØ¹\",\"Ø§Ù„Ù…\",\"Ø®Ø³Ø§Ø±Ø©\",\"ÙÙ‚Ø¯\"],\n",
        "    \"Distract\": [\"ØºÙŠØ± Ø°ÙŠ ØµÙ„Ø©\", \"ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù†Ø¸Ø§Ø±\", \"Ù…ÙˆØ¶ÙˆØ¹ Ø¢Ø®Ø±\", \"Ø¨Ø¹ÙŠØ¯ Ø¹Ù†\", \"Ù„Ø§ ÙŠØ±ØªØ¨Ø·\"]\n",
        "}\n",
        "\n",
        "def normalized_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "def classify_narrative(content):\n",
        "    if pd.isna(content) or not isinstance(content, str) or content.strip() == \"\":\n",
        "        return None\n",
        "    content = normalized_text(content)\n",
        "    for label, keywords in narrative_keywords.items():\n",
        "        if any(keyword in content for keyword in keywords):\n",
        "            return label\n",
        "    return \"Natural\"\n",
        "\n",
        "df[\"Narrative Manipulation (Comment)\"] = df[text_column].apply(classify_narrative)\n",
        "\n",
        "output_path = \"/content/comment/narrative_manipulation_comment.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Narrative manipulation analysis complete. Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BFKZmHyEDTs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/comment/narrative_manipulation_comment.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "against_keywords = [\n",
        "    \"Ø§Ù„ØºØ§Ø´Ù…\", \"Ù„Ø§ ÙŠÙ…ÙƒÙ†\", \"Ø¹Ù†ÙŠÙ\", \"Ø§Ù„Ø¹Ø¯ÙˆØ§Ù†\",\n",
        "    \"ØªØ­Ø°ÙŠØ±\", \"Ø®Ø·Ø±\", \"Ù…Ø´ÙƒÙ„Ø©\", \"Ø§Ù†ØªÙ‚Ø§Ø¯\", \"Ø³Ù„Ø¨ÙŠ\", \"Ø³Ù‚ÙˆØ·\",\"Ø³Ù„Ø¨ÙŠ\",\"ØªÙ‚Ù„ÙŠÙ„\",\"ØªØ®ÙÙŠØ¶\"\n",
        "    \"Ù‡Ø¬ÙˆÙ…\", \"Ù…Ø¹Ø§Ø±Ø¶Ø©\", \"Ø§Ø­ØªØ¬Ø§Ø¬\", \"ØªÙ‡Ø¯ÙŠØ¯\", \"ØªØ­Ø±ÙŠØ¶\", \"Ù…Ø¹Ø§Ø¯Ø§Ø©\", \"Ø®Ø³Ø§Ø±Ø©\"\n",
        "]\n",
        "\n",
        "favor_keywords = [\n",
        "    \"Ø§Ù†ØªØµØ§Ø±\", \"Ø¯Ø¹Ù…\", \"Ù†Ø¬Ø§Ø­\", \"Ø¥Ø´Ø§Ø¯Ø©\", \"Ù…ÙˆØ§ÙÙ‚Ø©\", \"Ø§Ø­ØªÙØ§Ù„\", \"ØªØ£ÙŠÙŠØ¯\",\"ØªØ²ÙˆÙŠØ¯\",\"Ø³Ø±Ø¹Ø©\",\n",
        "    \"Ù…Ø³Ø§Ù†Ø¯Ø©\", \"Ø­ÙÙ„\", \"ÙØ±Ø­\", \"Ù†Ø´Ø±\", \"ØªÙ…ÙƒÙŠÙ†\", \"Ù…Ø¹Ù‚ÙˆÙ„\", \"ÙÙˆØ²\", \"Ø®Ø¯Ù…Ø©\", \"ÙŠØ®Ø¯Ù…\", \"ÙŠØ¯Ø¹Ù…\",\"Ø§ÙŠØ¬Ø§Ø¨ÙŠ\",\"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ\",\"ÙˆÙˆÙˆÙˆ\",\"ÙŠÙŠÙŠÙŠ\"\n",
        "]\n",
        "\n",
        "def label_post(row):\n",
        "    content = str(row['Comment Text'])\n",
        "    content_lower = content.lower() if content != 'nan' else \"\"\n",
        "\n",
        "    if any(keyword in content_lower for keyword in against_keywords):\n",
        "            return \"Against\"\n",
        "    elif any(keyword in content_lower for keyword in favor_keywords):\n",
        "            return \"Favor\"\n",
        "\n",
        "    return \"Natural\"\n",
        "\n",
        "df['Stance (comment)'] = df.apply(label_post, axis=1)\n",
        "\n",
        "output_path = \"/content/comment/labeled_narrative_manipulation_posts.xlsx\"\n",
        "df.to_excel(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9_gwsjoFGHk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "file_path = \"/content/comment/labeled_narrative_manipulation_posts.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "post_ids = df['Comment ID']\n",
        "post_contents = df['Comment Text'].fillna('').str.lower()\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(post_contents)\n",
        "\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "similar_posts = []\n",
        "for i in range(cosine_sim_matrix.shape[0]):\n",
        "    for j in range(i + 1, cosine_sim_matrix.shape[1]):\n",
        "        if cosine_sim_matrix[i, j] >= similarity_threshold:\n",
        "            similar_posts.append({\n",
        "                'Comment ID 1': post_ids[i],\n",
        "                'Comment ID 2': post_ids[j],\n",
        "                'Similarity Score': cosine_sim_matrix[i, j],\n",
        "                'Comment1 Text': post_contents[i],\n",
        "                'Comment2 Text': post_contents[j]\n",
        "            })\n",
        "\n",
        "similar_posts_df = pd.DataFrame(similar_posts)\n",
        "\n",
        "if not similar_posts_df.empty:\n",
        "    output_file_path = '/content/comment/similar_posts_results_full_content.xlsx'\n",
        "    similar_posts_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"Results saved to {output_file_path}\")\n",
        "else:\n",
        "    print(\"No highly similar posts detected based on the chosen threshold.\")\n",
        "if similar_posts:\n",
        "    similarity_scores = [entry['Similarity Score'] for entry in similar_posts]\n",
        "    min_similarity = min(similarity_scores)\n",
        "    max_similarity = max(similarity_scores)\n",
        "    avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "    print(f\"Min: {min_similarity:.2f}, Max: {max_similarity:.2f}, Avg: {avg_similarity:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVpD9kA6FVsq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "main_file_path = \"/content/comment/labeled_narrative_manipulation_posts.xlsx\"\n",
        "df_main = pd.read_excel(main_file_path, sheet_name='Sheet1')\n",
        "\n",
        "similarity_file_path = '/content/comment/similar_posts_results_full_content.xlsx'\n",
        "df_similarity = pd.read_excel(similarity_file_path)\n",
        "\n",
        "most_similar_dict = {}\n",
        "\n",
        "for _, row in df_similarity.iterrows():\n",
        "    post1, post2, sim_score = row['Comment ID 1'], row['Comment ID 2'], row['Similarity Score']\n",
        "\n",
        "    if post1 not in most_similar_dict or sim_score > most_similar_dict[post1][1]:\n",
        "        most_similar_dict[post1] = (post2, sim_score)\n",
        "\n",
        "    if post2 not in most_similar_dict or sim_score > most_similar_dict[post2][1]:\n",
        "        most_similar_dict[post2] = (post1, sim_score)\n",
        "\n",
        "df_main['Most Similar Comment ID'] = df_main['Comment ID'].map(lambda x: most_similar_dict.get(x, (None, None))[0])\n",
        "df_main['Highest Similarity Score'] = df_main['Comment ID'].map(lambda x: most_similar_dict.get(x, (None, None))[1])\n",
        "\n",
        "df_main['Most Similar Comment ID'].fillna('No Match', inplace=True)\n",
        "df_main['Highest Similarity Score'].fillna(0, inplace=True)\n",
        "\n",
        "output_file = '/content/comment/updated_post_narrative_with_similarity.xlsx'\n",
        "df_main.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7km7F-0EF0WV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/comment/updated_post_narrative_with_similarity.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def elaboration_likelihood_model(content):\n",
        "    if pd.isna(content):\n",
        "        return \"Other\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"ØªØµØ±ÙŠØ­Ø§Øª\", \"Ø§Ù„ØªØµØ±ÙŠØ­Ø§Øª\", \"ØªØµØ±ÙŠØ­\", \"Ø¨ÙŠØ§Ù†\", \"Ø£Ø®Ø¨Ø§Ø±\", \"Ø§Ù„ØªØµØ±ÙŠØ­\", \"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\", \"Ø§Ù„Ø±Ø³Ø§Ù„Ø©\", \"Ø§Ù„ØªÙØ§ØµÙŠÙ„\", \"Ø§Ù„Ø­Ù‚\", \"ÙŠØ´Ø±Ø­\", \"ÙƒÙŠÙ\", \"Ù…Ø§ Ù…Ø¹Ù†Ù‰\"]):\n",
        "        return \"Information\"\n",
        "    elif any(keyword in content for keyword in [\"?\", \"Ù‡Ù„\", \"Ù…Ø§Ø°Ø§ Ù„Ùˆ\", \"??\", \"ØŸØŸ\", \"ØŸ\", \"!\", \"!!\"]):\n",
        "        return \"Question\"\n",
        "    elif any(keyword in content for keyword in [\"Ø¯Ø¹Ù…\", \"ØªØªØ¨Ø±Ø¹\", \"Ù…Ø¸Ø§Ù‡Ø±Ø©\", \"Ø´Ø§Ø±Ùƒ\", \"Ø¨Ù…Ø´Ø§Ø±ÙƒØ©\", \"ØªØ¨Ø±Ø¹\", \"ÙˆØ§Ù„ØªØ¨Ø±Ø¹\", \"Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©\", \"Ø¯Ø¹Ù…Ø§\", \"Ø§Ù„ØªØ¨Ø±Ø¹\", \"Ù„ØªØ¨Ø±Ø¹\", \"Ù…Ø´Ø§Ø±ÙƒØªÙ‡\", \"Ø¨ØªØ¨Ø±Ø¹\", \"Ø§Ù„ØªØ¨Ø±Ø¹Ø§Øª\", \"Ù„Ù„Ù…Ø´Ø§Ø±ÙƒØ©\", \"ÙˆØ¯Ø¹Ù…Ø§\", \"Ø¨Ø§Ù„ØªØ¨Ø±Ø¹\", \"Ø§Ù„Ø¯Ø¹Ù…\", \"ÙŠØ¯Ø¹Ù…\", \"ÙŠØ´Ø§Ø±Ùƒ\", \"ÙŠØ³Ø§Ù†Ø¯\", \"Ø§Ø³Ù†Ø§Ø¯\", \"Ø¥Ø³Ù†Ø§Ø¯\", \"Ø¯Ø¹Ù…Ø§Ù‹\", \"Ù…Ø´Ø§Ø±ÙƒØ©\"]):\n",
        "        return \"Participation\"\n",
        "    elif any(keyword in content for keyword in [\"Ø²Ø¹ÙŠÙ…\", \"Ù„Ù‚Ø§Ø¦Ø¯\", \"Ø³ÙŠØ§Ø³ÙŠ\", \"Ù‚Ø§Ø¦Ø¯\", \"Ù‚Ø§Ø¦Ø¯Ù†Ø§\", \"Ø§Ù„Ù‚Ø§Ø¦Ø¯\", \"Ù‚Ø§Ø¯Ø©\", \"Ø§Ù„Ù‚Ø§Ø¯Ø©\", \"Ù„Ù„Ù‚Ø§Ø¦Ø¯\", \"Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ\", \"Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ©\", \"Ù…Ø´Ø§Ù‡ÙŠØ±\", \"Ø§Ù„Ø±Ø¦ÙŠØ³\", \"Ø§Ù„Ø³Ù†ÙˆØ§Ø±\", \"Ø§Ø¨Ùˆ Ø¹Ø¨ÙŠØ¯Ø©\", \"Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡\", \"Ø§Ù„Ù‚Ø³Ø§Ù…\", \"Ø­Ù…Ø§Ø³\", \"Ù†ØµØ±Ø§Ù„Ù„Ù‡\"]):\n",
        "        return \"Celebrity\"\n",
        "    elif any(keyword in content for keyword in [\"Ù†ØµØ±Ø©\", \"Ù†ØµØ±Ø§Ù„Ù„Ù‡\", \"Ø¯Ø¹Ø§Ø¡\", \"Ù†ØµØ±\", \"Ø¥Ù„Ù‡Ø§Ù…\", \"Ø§Ù„Ù†ØµØ±\", \"Ø£Ù…Ù„\", \"Ø§Ù…Ø§Ù†ÙŠ\", \"Ø£Ù…Ø§Ù†ÙŠ\", \"Ø§Ù„Ù…ÙØ§Ø¬Ø£Ø©\"]):\n",
        "        return \"Inspiration\"\n",
        "    elif any(keyword in content for keyword in [\"Ø¶Ø­Ùƒ\", \"Ù…Ø²Ø§Ø­\", \"Ø³Ø®Ø±ÙŠØ©\", \"Ø§Ù„Ù…Ø¶Ø­Ùƒ\", \"Ù„Ù„Ø³Ø®Ø±ÙŠØ©\", \"Ø¨ØªÙ…Ø³Ø®Ø±Ùˆ\", \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\", \"Ù‡Ù‡Ù‡Ù‡Ù‡\", \"ha\", \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\", \"Ø¶Ø­ÙƒØªÙ†ÙŠ\", \"Ù…Ø³Ø®Ø±Ø©\", \"Ù‡Ù‡Ù‡\", \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\"]):\n",
        "        return \"Humor/Sarcasm\"\n",
        "    elif any(keyword in content for keyword in [\"ÙˆÙ…Ø´Ø§Ù‡Ø¯\", \"Ø´Ù‡Ø§Ø¯Ø©\", \"Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©\", \"Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯\", \"Ø±ÙˆØ§ÙŠØ©\", \"Ø´Ø§Ù‡Ø¯\", \"Ù…Ø´Ø§Ù‡Ø¯\", \"Ù‚ØµØ©\", \"Ù…Ø´Ù‡Ø¯\", \"Ø§Ù„Ø­Ù„Ù…\", \"Ø±ÙˆØ§ÙŠØ©\"]):\n",
        "        return \"Anecdotal/Story\"\n",
        "    else:\n",
        "        return \"Information\"\n",
        "\n",
        "df['elaboration_likelihood (Comment)'] = df['Comment Text'].apply(elaboration_likelihood_model)\n",
        "\n",
        "output_file = '/content/comment/categorized_posts.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJB0MKaHGBnr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset4.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def social_judgment_theory(content):\n",
        "    if pd.isna(content):\n",
        "        return \"\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\"Ø¯ÙˆØ§Ø¡\", \"ØµØ­Ø©\", \"Ù…Ø±Ø¶\", \"Ø´ÙØ§Ø¡\", \"Ø¹Ù„Ø§Ø¬\",\"Ø·Ø¨\",\"ØªÙ…Ø±ÙŠØ¶\",\"Ø§ØµØ§Ø¨Ø©\"]):\n",
        "        return \"Health/Evidence-based Health Information\"\n",
        "    elif any(keyword in content for keyword in [\"Ø§Ù„Ù†Ø¨ÙŠ\", \"Ø§Ù„Ù‚Ø±Ø¢Ù†\", \"Ø§Ù„Ø¥Ø³Ù„Ø§Ù…\", \"Ø¯ÙŠÙ†\", \"ØµÙ„Ø§Ø©\", \"Ø§Ù„Ø¯ÙŠÙ†\", \"Ø§Ù„ÙŠÙ‡ÙˆØ¯ÙŠØ©\",\n",
        "                                                \"Ø§Ù„Ø¬Ù†Ø©\", \"Ø§Ù„ÙŠÙ‡ÙˆØ¯\", \"Ù…Ø³Ù„Ù…ÙŠÙ†\", \"Ù…Ø³Ù„Ù…ÙˆÙ†\", \"Ø¥Ø³Ù„Ø§Ù…\",\"Ø§Ù„Ù…Ø³ÙŠØ­ÙŠØ©\"]):\n",
        "        return \"Religion\"\n",
        "    elif any(keyword in content for keyword in [\"Ø§Ø®ØªÙŠØ§Ø±\", \"Ù‚Ø±Ø§Ø±\", \"Ø­Ø±ÙŠØ©\", \"Ù†Ø¸Ø§Ù…\", \"Ø­Ù‚ÙˆÙ‚\", \"Ø±ÙØ§Ù‡ÙŠØ©\",\"Ø§Ù…ÙƒØ§Ù†ÙŠØ©\",\"Ø®ÙŠØ§Ø±\"]):\n",
        "        return \"Choice\"\n",
        "    elif any(keyword in content for keyword in [\"Ø­ÙƒÙˆÙ…Ø©\", \"Ø³ÙŠØ§Ø³ÙŠ\", \"Ø§Ø­ØªÙ„Ø§Ù„\", \"Ø¯ÙˆÙ„Ø©\", \"Ø¬ÙŠØ´\", \"Ø«ÙˆØ±Ø©\", \"Ø§Ù„Ø¯ÙˆÙ„Ø©\",\n",
        "                                                \"Ø³ÙŠØ§Ø³Ø©\", \"Ø§Ù„Ø³ÙŠØ§Ø³Ø©\", \"Ø§Ù„Ù…Ø¹ØªÙ‚Ù„ÙŠÙ†\",\"ØªØ±Ø§Ù…Ø¨\",\"Ø¨Ø§ÙŠØ¯Ù†\",\"Ø¯ÙˆÙ„ÙŠ\"]):\n",
        "        return \"Political\"\n",
        "    elif any(keyword in content for keyword in [\"Ø§Ù„Ù„Ù‡\",\"ØªØ¶Ø­ÙŠØ©\", \"Ø¥ÙŠØ«Ø§Ø±\", \"ÙƒØ±Ù…\", \"Ù†ØµØ±\", \"Ø¯Ø¹Ù…\"]):\n",
        "        return \"Altruism\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['social_judgment (comment)'] = df['Comment Text'].apply(social_judgment_theory)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset5.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Categorized posts saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8t5Xu5oHGgC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset5.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def classify_eppm(content):\n",
        "    if pd.isna(content):\n",
        "        return \"\"\n",
        "\n",
        "    if any(keyword in content for keyword in [\n",
        "        \"Ø®Ø·Ø±\", \"ÙƒØ§Ø±Ø«Ø©\", \"ØªÙ‡Ø¯ÙŠØ¯\", \"Ø®Ø³Ø§Ø±Ø©\", \"Ø¯Ù…Ø§Ø±\", \"Ù…Ø£Ø³Ø§Ø©\", \"Ù…ÙˆØª\", \"ÙˆÙÙŠØ§Øª\",\n",
        "        \"Ù‚ØµÙ\", \"Ø¯Ù…Ø§Ø¡\", \"Ø­Ø²Ù†\", \"Ø¹Ø°Ø§Ø¨\", \"Ù…Ø¹Ø§Ù†Ø§Ø©\", \"ÙƒØ§Ø±Ø«Ø©\", \"Ù…Ø¬Ø²Ø±Ø©\", \"Ø®Ø±Ø§Ø¨\",\n",
        "        \"ÙÙ‚Ø¯\", \"Ø¬Ø±Ø§Ø­\", \"Ù†Ø§Ø±\", \"Ù‡Ù„Ø§Ùƒ\", \"Ø¸Ù„Ù…\", \"Ø£Ù„Ù…\", \"Ø¶ÙŠØ§Ø¹\"\n",
        "    ]):\n",
        "        return \"Perceived Severity (EPPM-1)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"Ù‚Ø¯ ÙŠØµÙŠØ¨\", \"Ù…Ø¹Ø±Ø¶\", \"ÙŠØµÙŠØ¨\", \"Ù…Ø±Ø¶\", \"Ø£Ø«Ø±\", \"ÙŠØµØ§Ø¨ÙˆÙ†\", \"Ø®ÙˆÙ\", \"Ù‚Ù„Ù‚\",\n",
        "        \"Ø¶Ø¹Ù\", \"Ø®Ø·Ø± Ø¯Ø§Ù‡Ù…\", \"ÙŠØªØ¹Ø±Ø¶\", \"ØªÙ‡Ø¯ÙŠØ¯Ø§Øª\", \"ÙÙ‚Ø¯Ø§Ù†\", \"ÙŠÙ‡Ø¯Ø¯\", \"Ù…Ù‡Ø¯Ø¯\",\n",
        "        \"Ø§Ù†Ù‡ÙŠØ§Ø±\", \"Ø®Ø·Ø± Ù…Ø­Ø¯Ù‚\", \"Ù‚Ø±ÙŠØ¨\", \"Ø¶Ø­ÙŠØ©\",\"ØªÙˆØªØ±\"\n",
        "    ]):\n",
        "        return \"Perceived Susceptibility (EPPM-2)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"Ù‚Ø§Ø¯Ø±\", \"ÙŠÙ…ÙƒÙ†\", \"ÙŠØ³ØªØ·ÙŠØ¹\", \"Ù†Ø³ØªØ·ÙŠØ¹\", \"Ø­Ù„\", \"Ø§Ù…ÙƒØ§Ù†ÙŠØ©\", \"Ø§Ù„Ù‚Ø¯Ø±Ø©\", \"Ø§Ù„ØªØµØ¯ÙŠ\",\n",
        "        \"Ù†ØµÙ…Ø¯\", \"Ù†Ù†ØªØµØ±\", \"Ù†Ø­Ù…ÙŠ\", \"Ù†Ù‚Ù\", \"Ø«Ø¨Ø§Øª\", \"Ø¹Ø²Ù…\", \"ØªØµØ¯ÙŠ\", \"Ù…ÙˆØ§Ø¬Ù‡Ø©\",\n",
        "        \"ØªØ­Ù‚ÙŠÙ‚\", \"ØµÙ…ÙˆØ¯\", \"Ø¥ØµØ±Ø§Ø±\", \"Ù†ØªÙ‚Ø¯Ù…\", \"Ù†Ø¬Ø§Ø­\"\n",
        "    ]):\n",
        "        return \"Perceived Self-Efficacy (EPPM-3)\"\n",
        "\n",
        "    elif any(keyword in content for keyword in [\n",
        "        \"ÙØ¹Ø§Ù„Ø©\", \"ÙØ¹Ø§Ù„ÙŠØ©\", \"Ù†Ø¬Ø§Ø­\", \"Ù†ØªÙŠØ¬Ø©\", \"Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©\", \"Ø£ÙØ¶Ù„\", \"Ù…Ù†Ø§Ø³Ø¨\",\n",
        "        \"ØªØ­Ø±ÙŠØ±\", \"Ø§Ù†ØªØµØ§Ø±\", \"Ø¬Ø¯ÙˆÙ‰\", \"Ø±Ø¯ Ù‚ÙˆÙŠ\", \"ØªØ£Ø«ÙŠØ±\", \"Ø­Ù„ÙˆÙ„ Ù†Ø§Ø¬Ø­Ø©\", \"Ù†ØªØ§Ø¦Ø¬ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©\",\n",
        "        \"Ø£Ù…Ù„\", \"ÙØ§Ø¦Ø¯Ø©\", \"ØªØ­Ø³Ù†\", \"Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙØ¹Ø§Ù„Ø©\", \"Ù†ØµØ±\", \"Ø¨Ø°Ù„ Ø§Ù„Ø¬Ù‡ÙˆØ¯\", \"Ø«Ù…Ø§Ø±\",\"ÙÙˆØ²\"\n",
        "    ]):\n",
        "        return \"Perceived Response-Efficacy (EPPM-4)\"\n",
        "\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['EPPM Category (Comment)'] = df['Comment Text'].apply(classify_eppm)\n",
        "\n",
        "output_file = 'semi_final_Stage4_dataset6.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Classified EPPM posts saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMzVRyHjHhSe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "posts_file = 'posts_after_edits.xlsx'\n",
        "comments_file = 'comments_after_edits.xlsx'\n",
        "\n",
        "posts_df = pd.read_excel(posts_file)\n",
        "comments_df = pd.read_excel(comments_file)\n",
        "\n",
        "merged_df = posts_df.merge(comments_df, on='Post ID', how='left')\n",
        "\n",
        "merged_file = 'semi_final_Stage4_dataset.xlsx'\n",
        "merged_df.to_excel(merged_file, index=False)\n",
        "\n",
        "print(f\"Merged dataset saved to: {merged_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeRRsfQKiUHP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "post_columns = ['Post ID', 'Post Content', 'Post Time', 'Views', 'Forwards', 'Fire (Post)',\n",
        "                'Crying Face', 'Pray (Post)', 'Trophy (Post)', 'Clapping (Post)', 'Party (Post)',\n",
        "                'With Heart', 'Thumbs Up', 'Heart (Post)', 'Word Count', 'Words per Manip',\n",
        "                'Label', 'elaboration_likelihood', 'EPPM Category']\n",
        "\n",
        "df.loc[df.duplicated(subset=['Post ID'], keep='first'), post_columns] = ''\n",
        "\n",
        "output_file = 'cleaned_full_dataset.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrWdMfiwjyjN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'semi_final_Stage4_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "post_columns = ['Post ID', 'Post Content', 'Post Time', 'Views', 'Forwards', 'Fire (Post)',\n",
        "                'Crying Face', 'Pray (Post)', 'Trophy (Post)', 'Clapping (Post)', 'Party (Post)',\n",
        "                'With Heart', 'Thumbs Up', 'Heart (Post)', 'Word Count', 'Words per Manip',\n",
        "                'Label', 'ation_like_label', 'PM Categ']\n",
        "\n",
        "comment_counts = df.groupby('Post Content').size().reset_index(name='Comment Count')\n",
        "\n",
        "df = df.merge(comment_counts, on='Post Content', how='left')\n",
        "\n",
        "df.loc[df.duplicated(subset=['Post Content'], keep='first'), post_columns] = ''\n",
        "\n",
        "output_file = 'cleaned_dataset.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpuS7xN-bEec"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'new_full_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "columns_to_extract = ['Post Content', 'Post ID']\n",
        "df_extracted = df[columns_to_extract]\n",
        "\n",
        "output_path = 'extracted_posts.xlsx'\n",
        "df_extracted.to_excel(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqg49FUEv1bm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHlKUgGxyvZY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "file_path = \"extracted_posts.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "df = df[['Post Content']].dropna().reset_index(drop=True)\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
        "    text = re.sub(r'Ø©', 'Ù‡', text)\n",
        "    text = re.sub(r'ÙŠ', 'Ù‰', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned Post\"] = df[\"Post Content\"].apply(clean_arabic_text)\n",
        "\n",
        "def label_post(text):\n",
        "    central_keywords = [\"Ø¯Ø±Ø§Ø³Ø©\", \"Ø¨Ø­Ø«\", \"Ø¥Ø­ØµØ§Ø¦ÙŠØ©\", \"Ø¨ÙŠØ§Ù†Ø§Øª\", \"Ø¥Ø­ØµØ§Ø¡\", \"Ù„Ø£Ù†\", \"Ø¨Ø³Ø¨Ø¨\", \"Ø¥Ø°Ø§\", \"Ø¨Ø­Ø³Ø¨\", \"ÙˆÙÙ‚Ù‹Ø§ Ù„\"]\n",
        "\n",
        "    peripheral_keywords = [\"ÙƒØ§Ø±Ø«ÙŠ\", \"Ù…Ø®ÙŠÙ\", \"Ø®Ø·ÙŠØ±\", \"Ù…ØµÙŠØ¨Ø©\", \"Ù…Ø±Ø¹Ø¨\", \"ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡\", \"Ø¥Ù…Ø§ Ù…Ø¹Ù†Ø§ Ø£Ùˆ Ø¶Ø¯Ù†Ø§\", \"ØªÙ‡Ø¯ÙŠØ¯ ÙˆØ¬ÙˆØ¯ÙŠ\"]\n",
        "\n",
        "    if any(word in text for word in central_keywords):\n",
        "        return \"Central Cue\"\n",
        "    elif any(word in text for word in peripheral_keywords):\n",
        "        return \"Peripheral Cue\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "df[\"Label\"] = df[\"Cleaned Post\"].apply(label_post)\n",
        "\n",
        "df = df[df[\"Label\"] != \"Neutral\"].reset_index(drop=True)\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"Cleaned Post\"])\n",
        "y = df[\"Label\"].map({\"Central Cue\": 0, \"Peripheral Cue\": 1})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "df[\"Predicted Label\"] = model.predict(vectorizer.transform(df[\"Cleaned Post\"]))\n",
        "df[\"Predicted Label\"] = df[\"Predicted Label\"].map({0: \"Central Cue\", 1: \"Peripheral Cue\"})\n",
        "\n",
        "df.to_excel(\"classified_posts.xlsx\", index=False)\n",
        "print(\"Classification Completed! Results saved to classified_posts.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWcH_nYe2UiX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
