{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsDExMPpMg6H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import json\n",
        "import torchvision.ops\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import functional as FT\n",
        "from torchvision import transforms as T\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVg87VlSM2A7"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnupgnZnO_Ga"
      },
      "outputs": [],
      "source": [
        "def get_transforms(train=False):\n",
        "    if train:\n",
        "        transform = A.Compose([\n",
        "            A.Resize(600, 600),\n",
        "            A.HorizontalFlip(p=0.3),\n",
        "            A.VerticalFlip(p=0.3),\n",
        "            A.RandomBrightnessContrast(p=0.1),\n",
        "            A.ColorJitter(p=0.1),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='coco'))\n",
        "    else:\n",
        "        transform = A.Compose([\n",
        "            A.Resize(600, 600),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='coco'))\n",
        "    return transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG8xe9HUPSOy"
      },
      "outputs": [],
      "source": [
        "class UI_Element_Detection(datasets.VisionDataset):\n",
        "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "        self.split = split\n",
        "        self.coco = COCO(os.path.join(root, split, \"annotations_train.json\"))\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
        "\n",
        "    def _load_image(self, id: int):\n",
        "        path = self.coco.loadImgs(id)[0]['file_name']\n",
        "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        return image\n",
        "\n",
        "    def _load_target(self, id):\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        target = self._load_target(id)\n",
        "        target = copy.deepcopy(self._load_target(id))\n",
        "\n",
        "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image, bboxes=boxes)\n",
        "\n",
        "        image = transformed['image']\n",
        "        boxes = transformed['bboxes']\n",
        "\n",
        "        new_boxes = []\n",
        "        for box in boxes:\n",
        "            xmin = box[0]\n",
        "            xmax = xmin + box[2]\n",
        "            ymin = box[1]\n",
        "            ymax = ymin + box[3]\n",
        "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
        "        category_id_to_index = {cat_id: idx for idx, (cat_id, cat_info) in enumerate(categories.items())}\n",
        "\n",
        "        targ = {}\n",
        "        targ['boxes'] = boxes\n",
        "        targ['labels'] = torch.tensor([category_id_to_index[t['category_id']] for t in target], dtype=torch.int64)\n",
        "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
        "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
        "        return image.div(255), targ\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-tK7i6xPWYx"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/My Drive/MASC_UI_dataset/MASC_UI/MASC_UI\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf8ylOAiQ95o"
      },
      "outputs": [],
      "source": [
        "#load classes\n",
        "coco = COCO(os.path.join(dataset_path, \"train\", \"annotations_train.json\"))\n",
        "categories = coco.cats\n",
        "n_classes = len(categories.keys())\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpsVGzQsQ9yD"
      },
      "outputs": [],
      "source": [
        "classes = [i[1]['name'] for i in categories.items()]\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X57IozmjQ9pa"
      },
      "outputs": [],
      "source": [
        "train_dataset = UI_Element_Detection(root=dataset_path, transforms=get_transforms(True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck9vU9tzyPUm"
      },
      "outputs": [],
      "source": [
        "val_dataset = UI_Element_Detection(root=dataset_path, split=\"val\", transforms=get_transforms(False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOsvMaekQ2LS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "\n",
        "def get_file_name(coco, image_id):\n",
        "    return coco.loadImgs(image_id)[0]['file_name']\n",
        "\n",
        "\n",
        "sample_index = 333\n",
        "sample = train_dataset[sample_index]\n",
        "img_tensor = sample[0] * 255\n",
        "img_int = img_tensor.to(torch.uint8)\n",
        "\n",
        "\n",
        "image_id = sample[1]['image_id'][0].item()\n",
        "file_name = get_file_name(train_dataset.coco, image_id)\n",
        "\n",
        "\n",
        "new_width, new_height = img_tensor.shape[2] * 2, img_tensor.shape[1] * 2\n",
        "img_resized = F.resize(img_int, [new_height, new_width])\n",
        "\n",
        "\n",
        "boxes_resized = sample[1]['boxes'] * torch.tensor([2, 2, 2, 2], dtype=torch.float32)\n",
        "\n",
        "\n",
        "img_with_boxes = draw_bounding_boxes(\n",
        "    img_resized, boxes_resized, [classes[i] for i in sample[1]['labels']], width=4\n",
        ")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img_with_boxes.permute(1, 2, 0))\n",
        "\n",
        "for i, label in enumerate(sample[1]['labels']):\n",
        "    box = boxes_resized[i]\n",
        "    category_id = label.item()\n",
        "    category_name = classes[category_id]\n",
        "    plt.text(box[0], box[1], f\"{category_name}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "print(f\"File Name: {file_name}\")\n",
        "\n",
        "for i, label in enumerate(sample[1]['labels']):\n",
        "    box = boxes_resized[i]\n",
        "    category_id = label.item()\n",
        "    print(f\"Bounding Box {i}: {box}, Category ID: {category_id}, Category Name: {classes[category_id]}\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCoytXmoQCda"
      },
      "outputs": [],
      "source": [
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
        "\n",
        "# model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKc6MT7tQD_p"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZrUbEwjOWmF"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKoY309_zLdB"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEkBPNaIOemI"
      },
      "outputs": [],
      "source": [
        "images,targets = next(iter(train_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k:v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUYbWzWeOkRh"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKEiQ3PlOnC_"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJSYYcRoOpJE"
      },
      "outputs": [],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.007, momentum=0.9, nesterov=True, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S0yF-AdOqpB"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qwcl012OvAf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    all_losses = []\n",
        "    all_losses_dict = []\n",
        "\n",
        "    for images, targets in tqdm(loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        all_losses.append(loss_value)\n",
        "        all_losses_dict.append(loss_dict_append)\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    all_losses_dict = pd.DataFrame(all_losses_dict)\n",
        "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
        "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
        "        all_losses_dict['loss_classifier'].mean(),\n",
        "        all_losses_dict['loss_box_reg'].mean(),\n",
        "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
        "        all_losses_dict['loss_objectness'].mean()\n",
        "    ))\n",
        "\n",
        "    return all_losses, all_losses_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdiWqZ10_v5k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "print(f\"Total training time: {total_duration:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ocvO4yIzmn5"
      },
      "outputs": [],
      "source": [
        "epoch_losses_df = pd.DataFrame(epoch_losses_dict)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(num_epochs), epoch_losses, label='Total Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_classifier'], label='Loss Classifier')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_box_reg'], label='Loss Box Reg')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_rpn_box_reg'], label='Loss RPN Box Reg')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_objectness'], label='Loss Objectness')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Individual Loss Components Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdDYK6oDWWpF"
      },
      "outputs": [],
      "source": [
        "def check_class_distribution(loader):\n",
        "    class_counts = [0] * 5\n",
        "\n",
        "    for _, targets in loader:\n",
        "        for target in targets:\n",
        "            labels = target['labels'].numpy()\n",
        "            for label in labels:\n",
        "                class_counts[label] += 1\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "train_class_distribution = check_class_distribution(train_loader)\n",
        "val_class_distribution = check_class_distribution(val_loader)\n",
        "\n",
        "print(f\"Train class distribution: {train_class_distribution}\")\n",
        "print(f\"Validation class distribution: {val_class_distribution}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6zU3J9yDK74"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "def iou(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x1g, y1g, x2g, y2g = box2\n",
        "\n",
        "    xi1, yi1 = max(x1, x1g), max(y1, y1g)\n",
        "    xi2, yi2 = min(x2, x2g), min(y2, y2g)\n",
        "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
        "\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "def calculate_precision_recall(all_boxes, all_scores, all_labels, true_boxes, true_labels, iou_threshold=0.5):\n",
        "    if len(all_scores) == 0:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    sorted_indices = np.argsort(-np.array(all_scores))\n",
        "    all_boxes = np.array(all_boxes)[sorted_indices]\n",
        "    all_labels = np.array(all_labels)[sorted_indices]\n",
        "    all_scores = np.array(all_scores)[sorted_indices]\n",
        "\n",
        "    num_true_boxes = len(true_boxes)\n",
        "    num_pred_boxes = len(all_boxes)\n",
        "\n",
        "    true_positive = np.zeros(num_pred_boxes)\n",
        "    false_positive = np.zeros(num_pred_boxes)\n",
        "    detected_boxes = []\n",
        "\n",
        "    for idx, pred_box in enumerate(all_boxes):\n",
        "        max_iou = 0\n",
        "        max_idx = -1\n",
        "        for jdx, true_box in enumerate(true_boxes):\n",
        "            if all_labels[idx] == true_labels[jdx] and jdx not in detected_boxes:\n",
        "                current_iou = iou(pred_box, true_box)\n",
        "                if current_iou > max_iou:\n",
        "                    max_iou = current_iou\n",
        "                    max_idx = jdx\n",
        "\n",
        "        if max_iou >= iou_threshold:\n",
        "            true_positive[idx] = 1\n",
        "            detected_boxes.append(max_idx)\n",
        "        else:\n",
        "            false_positive[idx] = 1\n",
        "\n",
        "    cum_tp = np.cumsum(true_positive)\n",
        "    cum_fp = np.cumsum(false_positive)\n",
        "    precision = cum_tp / (cum_tp + cum_fp) if (cum_tp + cum_fp).sum() > 0 else np.array([])\n",
        "    recall = cum_tp / num_true_boxes if num_true_boxes > 0 else np.array([])\n",
        "\n",
        "    return precision, recall, all_scores\n",
        "\n",
        "def calculate_ap(precision, recall):\n",
        "    if len(precision) == 0 or len(recall) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    recall = np.concatenate(([0.0], recall, [1.0]))\n",
        "    precision = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    for i in range(len(precision) - 2, -1, -1):\n",
        "        precision[i] = max(precision[i], precision[i + 1])\n",
        "\n",
        "    indices = np.where(recall[1:] != recall[:-1])[0]\n",
        "    average_precision = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
        "\n",
        "    return average_precision\n",
        "\n",
        "def evaluate_model(loader, model, device, class_names, iou_thresh=0.5):\n",
        "    model.eval()\n",
        "    all_detections = []\n",
        "    all_annotations = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                scores = output['scores'].cpu().numpy()\n",
        "                labels = output['labels'].cpu().numpy()\n",
        "                boxes = output['boxes'].cpu().numpy()\n",
        "\n",
        "                target_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "                target_labels = targets[i]['labels'].cpu().numpy()\n",
        "\n",
        "                valid = scores > 0.5\n",
        "                scores = scores[valid]\n",
        "                labels = labels[valid]\n",
        "                boxes = boxes[valid]\n",
        "\n",
        "                all_detections.extend([(box, score, label) for box, score, label in zip(boxes, scores, labels)])\n",
        "                all_annotations.extend([(box, label) for box, label in zip(target_boxes, target_labels)])\n",
        "\n",
        "    if not all_detections:\n",
        "        return [], [], [], 0\n",
        "\n",
        "    pred_boxes, pred_scores, pred_labels = zip(*all_detections)\n",
        "    true_boxes, true_labels = zip(*all_annotations)\n",
        "\n",
        "    aps, precisions, recalls = [], [], []\n",
        "\n",
        "    for class_id in range(1, n_classes):\n",
        "        class_pred_boxes = [pred_boxes[i] for i in range(len(pred_labels)) if pred_labels[i] == class_id]\n",
        "        class_pred_scores = [pred_scores[i] for i in range(len(pred_labels)) if pred_labels[i] == class_id]\n",
        "        class_true_boxes = [true_boxes[i] for i in range(len(true_labels)) if true_labels[i] == class_id]\n",
        "\n",
        "        precision, recall, _ = calculate_precision_recall(class_pred_boxes, class_pred_scores, [class_id]*len(class_pred_boxes), class_true_boxes, [class_id]*len(class_true_boxes), iou_threshold=iou_thresh)\n",
        "        ap = calculate_ap(precision, recall)\n",
        "\n",
        "        if len(precision) == 0:\n",
        "            print(f\"No predictions for {class_names[class_id-1]}.\")\n",
        "        if len(recall) == 0:\n",
        "            print(f\"No true positives for {class_names[class_id-1]}.\")\n",
        "\n",
        "        aps.append(ap)\n",
        "        precisions.append(np.mean(precision) if len(precision) > 0 else float('nan'))\n",
        "        recalls.append(np.mean(recall) if len(recall) > 0 else float('nan'))\n",
        "\n",
        "    mean_ap = np.mean(aps)\n",
        "    return precisions, recalls, aps, mean_ap\n",
        "\n",
        "n_classes = len(categories.keys())\n",
        "classes = [i[1]['name'] for i in categories.items()]\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 70\n",
        "\n",
        "epoch_losses = []\n",
        "epoch_losses_dict = []\n",
        "\n",
        "total_start_time = time.time()\n",
        "\n",
        "train_mAPs, val_mAPs = [], []\n",
        "train_precisions, val_precisions = [], []\n",
        "train_recalls, val_recalls = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    losses, losses_dict = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "\n",
        "    train_precisions_epoch, train_recalls_epoch, train_aps, train_mean_ap = evaluate_model(train_loader, model, device, classes)\n",
        "    train_mAPs.append(train_mean_ap)\n",
        "    train_precisions.append(np.nanmean(train_precisions_epoch))\n",
        "    train_recalls.append(np.nanmean(train_recalls_epoch))\n",
        "    print(f\"Epoch {epoch} - Train mAP@0.50: {train_mean_ap}\")\n",
        "    for class_id, (precision, recall, ap) in enumerate(zip(train_precisions_epoch, train_recalls_epoch, train_aps), start=1):\n",
        "        print(f\"{classes[class_id-1]} - Precision: {precision}, Recall: {recall}, AP: {ap}\")\n",
        "\n",
        "    val_precisions_epoch, val_recalls_epoch, val_aps, val_mean_ap = evaluate_model(val_loader, model, device, classes)\n",
        "    val_mAPs.append(val_mean_ap)\n",
        "    val_precisions.append(np.nanmean(val_precisions_epoch))\n",
        "    val_recalls.append(np.nanmean(val_recalls_epoch))\n",
        "    print(f\"Epoch {epoch} - Validation mAP@0.50: {val_mean_ap}\")\n",
        "    for class_id, (precision, recall, ap) in enumerate(zip(val_precisions_epoch, val_recalls_epoch, val_aps), start=1):\n",
        "        print(f\"{classes[class_id-1]} - Precision: {precision}, Recall: {recall}, AP: {ap}\")\n",
        "\n",
        "    epoch_losses.append(np.mean(losses))\n",
        "    epoch_losses_dict.append(losses_dict.mean())\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "print(f\"Total training time: {total_duration:.2f} seconds\")\n",
        "\n",
        "epochs = range(num_epochs)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, train_mAPs, label='Train mAP')\n",
        "plt.plot(epochs, val_mAPs, label='Validation mAP')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('mAP')\n",
        "plt.title('mAP over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, train_precisions, label='Train Precision')\n",
        "plt.plot(epochs, val_precisions, label='Validation Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs, train_recalls, label='Train Recall')\n",
        "plt.plot(epochs, val_recalls, label='Validation Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "train_f1_scores = [2 * (p * r) / (p + r) for p, r in zip(train_precisions, train_recalls)]\n",
        "val_f1_scores = [2 * (p * r) / (p + r) for p, r in zip(val_precisions, val_recalls)]\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs, train_f1_scores, label='Train F1 Score')\n",
        "plt.plot(epochs, val_f1_scores, label='Validation F1 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVe3cP_1ObFt"
      },
      "outputs": [],
      "source": [
        "epoch_losses_df = pd.DataFrame(epoch_losses_dict)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(num_epochs), epoch_losses, label='Total Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_classifier'], label='Loss Classifier')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_box_reg'], label='Loss Box Reg')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_rpn_box_reg'], label='Loss RPN Box Reg')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_objectness'], label='Loss Objectness')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Individual Loss Components Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vHIPaXP8d8N"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alHVvL_48wcb"
      },
      "outputs": [],
      "source": [
        "test_dataset = UI_Element_Detection(root=dataset_path, split=\"test\", transforms=get_transforms(False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMM5J4mcQeUv"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da3oTs65Tu6l"
      },
      "outputs": [],
      "source": [
        "img, _ = test_dataset[45]\n",
        "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])\n",
        "    pred = prediction[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsjuTNr4-BFj"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(14, 10))\n",
        "plt.imshow(draw_bounding_boxes(img_int,\n",
        "    pred['boxes'][pred['scores'] > 0.5],\n",
        "    [classes[i] for i in pred['labels'][pred['scores'] > 0.5].tolist()], width=4\n",
        ").permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9pAEd1MC6pS"
      },
      "outputs": [],
      "source": [
        "pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPA2cRBZ67Mp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import easyocr\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n",
        "expert_dataset_path = '/content/drive/My Drive/MASC_UI_dataset/MASC_UI/MASC_UI/expert_dataset.xlsx'\n",
        "expert_df = pd.read_excel(expert_dataset_path)\n",
        "login_texts = expert_df['Login'].dropna().tolist()\n",
        "home_texts = expert_df['Home'].dropna().tolist()\n",
        "welcome_texts = expert_df['Welcome'].dropna().tolist()\n",
        "profile_texts = expert_df['Profile'].dropna().tolist()\n",
        "\n",
        "reader = easyocr.Reader(['en'])\n",
        "\n",
        "def load_image(uploaded):\n",
        "    image = Image.open(io.BytesIO(uploaded['content'])).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def preprocess_image_inference(image, transform):\n",
        "    image = np.array(image)\n",
        "    transformed = transform(image=image)\n",
        "    image_tensor = transformed['image']\n",
        "    return image_tensor.float() / 255.0\n",
        "\n",
        "def recognize_text(image, predictions, classes):\n",
        "    recognized_texts = []\n",
        "    boxes = predictions['boxes'][predictions['scores'] > 0.5].cpu().numpy()\n",
        "    labels = predictions['labels'][predictions['scores'] > 0.5].cpu().numpy()\n",
        "\n",
        "    for box, label in zip(boxes, labels):\n",
        "        class_name = classes[label]\n",
        "        if class_name == 'TextView':\n",
        "            x_min, y_min, x_max, y_max = map(int, box)\n",
        "            cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "            results = reader.readtext(cropped_image)\n",
        "            text_lines = \" \".join([result[1] for result in results])\n",
        "            recognized_texts.append((text_lines, box, class_name))\n",
        "            print(\"Detected text:\", text_lines)\n",
        "    return recognized_texts\n",
        "\n",
        "def visualize_results(image_tensor, predictions, classes):\n",
        "    img_np = (image_tensor * 255).to(torch.uint8).cpu().permute(1, 2, 0).numpy()\n",
        "    recognized_texts = recognize_text(img_np, predictions, classes)\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    plt.imshow(draw_bounding_boxes(\n",
        "        (image_tensor * 255).to(torch.uint8).cpu(),\n",
        "        predictions['boxes'][predictions['scores'] > 0.5].cpu(),\n",
        "        [classes[i] for i in predictions['labels'][predictions['scores'] > 0.5].tolist()],\n",
        "        width=4).permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return recognized_texts\n",
        "\n",
        "def check_ux_and_suggestions(detected_texts, labels):\n",
        "    detected_words = set(\" \".join([text for text, _, _ in detected_texts]).split())\n",
        "\n",
        "    expert_words = set()\n",
        "    for column_texts in [login_texts, home_texts, welcome_texts, profile_texts]:\n",
        "        for text in column_texts:\n",
        "            expert_words.update(text.split())\n",
        "\n",
        "    if detected_words & expert_words:\n",
        "        return \"Good UX\", None\n",
        "\n",
        "    detected_labels = set(labels)\n",
        "\n",
        "    if 'Input' in detected_labels and not any(word in detected_words for word in login_texts):\n",
        "        return \"Fault in UX - Login\", login_texts\n",
        "\n",
        "    if detected_labels == {'ImageView', 'TextView', 'Button'}:\n",
        "        return \"Fault in UX - Home\", home_texts\n",
        "\n",
        "    if detected_labels <= {'ImageView', 'Button'} and len(detected_labels) == 2:\n",
        "        return \"Fault in UX - Welcome\", welcome_texts\n",
        "\n",
        "    if not any(word in detected_words for word in profile_texts):\n",
        "        return \"Fault in UX - Home\", home_texts\n",
        "\n",
        "    return \"Good UX\", None\n",
        "\n",
        "def load_and_predict(change):\n",
        "    uploaded = list(file_picker.value.values())[0]\n",
        "    if not uploaded:\n",
        "        return\n",
        "\n",
        "    new_image = load_image(uploaded)\n",
        "    transform = A.Compose([\n",
        "        A.Resize(600, 600),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    image_tensor = preprocess_image_inference(new_image, transform)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        predictions = model([image_tensor])\n",
        "        pred = predictions[0]\n",
        "\n",
        "    recognized_texts = visualize_results(image_tensor, pred, classes)\n",
        "    detected_labels = [classes[label] for label in pred['labels'][pred['scores'] > 0.5].cpu().numpy()]\n",
        "    ux_result, suggestions = check_ux_and_suggestions(recognized_texts, detected_labels)\n",
        "    if ux_result != \"Good UX\" and suggestions:\n",
        "        result_html.value = f\"{ux_result}<br><br>Suggestions:<br>\" + \"<br>\".join(suggestions)\n",
        "    else:\n",
        "        result_html.value = ux_result\n",
        "\n",
        "file_picker = widgets.FileUpload(accept='image/*', multiple=False, description=\"Upload Image\", button_style='info')\n",
        "file_picker.observe(load_and_predict, names='value')\n",
        "\n",
        "result_html = widgets.HTML(value=\"\", layout=widgets.Layout(width='100%', height='200px', border='solid 1px black', padding='10px'))\n",
        "\n",
        "image_output = widgets.Output(layout=widgets.Layout(width='50%'))\n",
        "result_output = widgets.VBox([\n",
        "    widgets.HTML(\"<h2>UI Element Detection</h2>\"),\n",
        "    file_picker,\n",
        "    widgets.HTML(\"<h3>UX Evaluation Result:</h3>\"),\n",
        "    result_html\n",
        "], layout=widgets.Layout(align_items='center', width='50%', margin='0 auto'))\n",
        "\n",
        "def load_and_predict(change):\n",
        "    uploaded = list(file_picker.value.values())[0]\n",
        "    if not uploaded:\n",
        "        return\n",
        "\n",
        "    new_image = load_image(uploaded)\n",
        "    transform = A.Compose([\n",
        "        A.Resize(600, 600),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    image_tensor = preprocess_image_inference(new_image, transform)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        predictions = model([image_tensor])\n",
        "        pred = predictions[0]\n",
        "\n",
        "    with image_output:\n",
        "        image_output.clear_output()\n",
        "        recognized_texts = visualize_results(image_tensor, pred, classes)\n",
        "\n",
        "    detected_labels = [classes[label] for label in pred['labels'][pred['scores'] > 0.5].cpu().numpy()]\n",
        "    ux_result, suggestions = check_ux_and_suggestions(recognized_texts, detected_labels)\n",
        "    if ux_result != \"Good UX\" and suggestions:\n",
        "        result_html.value = f\"{ux_result}<br><br>Suggestions:<br>\" + \"<br>\".join(suggestions)\n",
        "    else:\n",
        "        result_html.value = ux_result\n",
        "\n",
        "file_picker.observe(load_and_predict, names='value')\n",
        "\n",
        "app_layout = widgets.HBox([\n",
        "    image_output,\n",
        "    result_output\n",
        "], layout=widgets.Layout(align_items='center', width='100%', margin='10 auto'))\n",
        "\n",
        "display(app_layout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBd_gwnWVkuT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
