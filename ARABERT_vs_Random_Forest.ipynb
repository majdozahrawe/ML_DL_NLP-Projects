{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2StrGwaCxIU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers pandas scikit-learn evaluate openpyxl numpy"
      ],
      "metadata": {
        "id": "-6Lxi-XnDYSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farasa\n",
        "!pip install arabic-reshaper\n",
        "!pip install wordcloud\n",
        "!pip install tashaphyne"
      ],
      "metadata": {
        "id": "JZ5ln3GmvClT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farasapy"
      ],
      "metadata": {
        "id": "Wem7jkY7v-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "file_path = \"comments_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df = df[['Comment Text']].dropna().reset_index(drop=True)\n",
        "\n",
        "# Initialize Arabic NLP Tools\n",
        "segmenter = FarasaSegmenter(interactive=True)\n",
        "stemmer = FarasaStemmer(interactive=True)\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.strip()\n",
        "    text = segmenter.segment(text)\n",
        "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned Post\"] = df[\"Comment Text\"].apply(clean_arabic_text)\n",
        "\n",
        "central_regex = r'\\b(ÿØÿ±ÿßÿ≥ÿ©|ÿ®ÿ≠ÿ´|ÿ®ŸäÿßŸÜÿßÿ™|ÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™|ŸÑÿ£ŸÜ|ŸÑÿ∞ŸÑŸÉ|ÿ®ÿßŸÑÿ™ÿßŸÑŸä|ÿ≠ÿ≥ÿ®|ŸàŸÅŸÇŸãÿß ŸÑ)\\b'\n",
        "peripheral_regex = r'\\b(ÿπÿßÿ¨ŸÑ|ÿÆÿ∑Ÿäÿ±|ŸÉÿßÿ±ÿ´Ÿä|ÿ£ÿ≤ŸÖÿ©|ÿßŸÑÿÆÿ®ÿ±ÿßÿ° ŸäŸÇŸàŸÑŸàŸÜ|ŸäÿØÿπŸàŸÜ|ÿ™ÿ∏Ÿáÿ± ÿßŸÑÿØÿ±ÿßÿ≥ÿßÿ™)\\b'\n",
        "\n",
        "# Function: Labeling Based on NLP Rules\n",
        "def label_post(text):\n",
        "    if re.search(central_regex, text):\n",
        "        return \"Central Cue\"\n",
        "    elif re.search(peripheral_regex, text) or TextBlob(text).sentiment.polarity > 0.5:\n",
        "        return \"Peripheral Cue\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "df[\"Label\"] = df[\"Cleaned Post\"].apply(label_post)\n",
        "df = df[df[\"Label\"] != \"Neutral\"].reset_index(drop=True)\n",
        "\n",
        "# TF-IDF for Feature Extraction\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"Cleaned Post\"])\n",
        "y = df[\"Label\"].map({\"Central Cue\": 0, \"Peripheral Cue\": 1})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM Model\n",
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classify New Posts\n",
        "df[\"Predicted Label\"] = model.predict(vectorizer.transform(df[\"Cleaned Post\"]))\n",
        "df[\"Predicted Label\"] = df[\"Predicted Label\"].map({0: \"Central Cue\", 1: \"Peripheral Cue\"})\n",
        "\n",
        "df.to_excel(\"classified_posts.xlsx\", index=False)\n",
        "print(\"‚úÖ Automated Cue Classification Completed!\")\n"
      ],
      "metadata": {
        "id": "Jt7o5k1o1AF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model, \"cue_classifier.pkl\")\n",
        "\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"‚úÖ Model and Vectorizer Saved Successfully!\")\n"
      ],
      "metadata": {
        "id": "Y6E6ax7vu9Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "import string\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "\n",
        "model = joblib.load(\"cue_classifier.pkl\")\n",
        "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "\n",
        "segmenter = FarasaSegmenter(interactive=True)\n",
        "stemmer = FarasaStemmer(interactive=True)\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.strip()\n",
        "    text = segmenter.segment(text)\n",
        "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "u_jMJDrWBtWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_file_path = \"comments_data.xlsx\"\n",
        "df_large = pd.read_excel(large_file_path)\n",
        "\n",
        "df_large = df_large[['Comment Text']].dropna().reset_index(drop=True)\n",
        "\n",
        "df_large[\"Cleaned Post\"] = df_large[\"Comment Text\"].apply(clean_arabic_text)\n",
        "\n",
        "X_large = vectorizer.transform(df_large[\"Cleaned Post\"])\n",
        "\n",
        "df_large[\"Predicted Label\"] = model.predict(X_large)\n",
        "\n",
        "df_large[\"Predicted Label\"] = df_large[\"Predicted Label\"].map({0: \"Central Cue\", 1: \"Peripheral Cue\"})\n",
        "\n",
        "df_large.to_excel(\"cues_labeled_comments_dataset.xlsx\", index=False)\n",
        "\n",
        "print(\"‚úÖ Large Dataset Classification Completed! Results saved in 'labeled_big_dataset.xlsx'.\")\n"
      ],
      "metadata": {
        "id": "5m7ibBxcBxsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "file_path = \"cues_labeled_comments_dataset.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "df = df.dropna(subset=['Comment Text'])\n",
        "\n",
        "def combine_labels(row):\n",
        "    return list(filter(pd.notna, [row[\"Predicted Label\"],]))\n",
        "\n",
        "df[\"Labels\"] = df.apply(combine_labels, axis=1)\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels_encoded = mlb.fit_transform(df[\"Labels\"])\n",
        "\n",
        "labels_df = pd.DataFrame(labels_encoded, columns=mlb.classes_)\n",
        "\n",
        "final_df = pd.concat([df[['Comment Text']].reset_index(drop=True), labels_df], axis=1)\n",
        "\n",
        "final_df.to_excel(\"formatted_labeled_cues_comment_dataset.xlsx\", index=False)\n",
        "\n",
        "print(\"‚úÖ Dataset formatted successfully and saved as formatted_comments_dataset.xlsx\")\n"
      ],
      "metadata": {
        "id": "nyRu3g50CBV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_cues = \"cues_post_data.xlsx\"\n",
        "file_propaganda = \"propaganda_post_data.xlsx\"\n",
        "\n",
        "df_cues = pd.read_excel(file_cues)\n",
        "df_propaganda = pd.read_excel(file_propaganda)\n",
        "\n",
        "df_merged = pd.merge(df_propaganda, df_cues, on=\"Post Content\", how=\"inner\")\n",
        "\n",
        "df_merged.to_excel(\"full_features_labeled_posts.xlsx\", index=False)\n",
        "\n",
        "print(\"‚úÖ Merged dataset saved as 'merged_dataset.xlsx'\")\n"
      ],
      "metadata": {
        "id": "ctcPxQYpU5-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "both features"
      ],
      "metadata": {
        "id": "QzVGyozXn0bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from evaluate import load\n",
        "\n",
        "file_path = \"full_features_labeled_posts.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    import re\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Processed_Text\"] = df[\"Post Content\"].astype(str).apply(preprocess_text)\n",
        "\n",
        "def integrate_features(row):\n",
        "    central = \"Central Cue: Yes.\" if row[\"Central Cue\"] == 1 else \"Central Cue: No.\"\n",
        "    peripheral = \"Peripheral Cue: Yes.\" if row[\"Peripheral Cue\"] == 1 else \"Peripheral Cue: No.\"\n",
        "    return f\"{central} {peripheral} {row['Processed_Text']}\"\n",
        "\n",
        "df[\"Final_Text\"] = df.apply(integrate_features, axis=1)\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"Final_Text\"].tolist(), df[\"Propaganda\"].tolist(),\n",
        "    test_size=0.3, stratify=df[\"Propaganda\"], random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels,\n",
        "    test_size=0.5, stratify=temp_labels, random_state=42\n",
        ")\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenizing Data\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "val_encodings = tokenize_function(val_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "class PropagandaDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = PropagandaDataset(train_encodings, train_labels)\n",
        "val_dataset = PropagandaDataset(val_encodings, val_labels)\n",
        "test_dataset = PropagandaDataset(test_encodings, test_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    eval_steps=50,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    learning_rate=0.00001,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "train_labels_np = np.array(train_labels)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels_np),\n",
        "    y=train_labels_np\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "accuracy_metric = load(\"accuracy\")\n",
        "precision_metric = load(\"precision\")\n",
        "recall_metric = load(\"recall\")\n",
        "f1_metric = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=None)[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=None)[\"recall\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=None)[\"f1\"]\n",
        "\n",
        "    precision_avg = np.mean(precision).item()\n",
        "    recall_avg = np.mean(recall).item()\n",
        "    f1_avg = np.mean(f1).item()\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"precision\": precision_avg,\n",
        "        \"recall\": recall_avg,\n",
        "        \"f1_score\": f1_avg\n",
        "    }\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"\\nüîç Test Set Evaluation Results:\")\n",
        "for metric, value in test_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "model.save_pretrained(\"arabert_propaganda_model_with_cues\")\n",
        "tokenizer.save_pretrained(\"arabert_propaganda_model_with_cues\")\n",
        "\n",
        "print(\"\\n‚úÖ Model Training and Evaluation Complete! Model Saved.\")\n"
      ],
      "metadata": {
        "id": "qH5mMGypDcUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "rS4JV_FcZinj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from evaluate import load\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import joblib\n",
        "\n",
        "file_path = \"full_features_labeled_posts.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    import re\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Processed_Text\"] = df[\"Post Content\"].astype(str).apply(preprocess_text)\n",
        "\n",
        "def integrate_features(row):\n",
        "    central = \"Central Cue: Yes.\" if row[\"Central Cue\"] == 1 else \"Central Cue: No.\"\n",
        "    peripheral = \"Peripheral Cue: Yes.\" if row[\"Peripheral Cue\"] == 1 else \"Peripheral Cue: No.\"\n",
        "    return f\"{central} {peripheral} {row['Processed_Text']}\"\n",
        "\n",
        "df[\"Final_Text\"] = df.apply(integrate_features, axis=1)\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"Final_Text\"].tolist(), df[\"Propaganda\"].tolist(),\n",
        "    test_size=0.3, stratify=df[\"Propaganda\"], random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels,\n",
        "    test_size=0.5, stratify=temp_labels, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "train_features = vectorizer.fit_transform(train_texts)\n",
        "val_features = vectorizer.transform(val_texts)\n",
        "test_features = vectorizer.transform(test_texts)\n",
        "\n",
        "train_labels_np = np.array(train_labels)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels_np),\n",
        "    y=train_labels_np\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=20,\n",
        "    class_weight=class_weights_dict,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(train_features, train_labels)\n",
        "\n",
        "val_preds = rf_model.predict(val_features)\n",
        "test_preds = rf_model.predict(test_features)\n",
        "\n",
        "def compute_rf_metrics(preds, labels):\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=None).mean(),\n",
        "        \"recall\": recall_score(labels, preds, average=None).mean(),\n",
        "        \"f1_score\": f1_score(labels, preds, average=None).mean()\n",
        "    }\n",
        "\n",
        "val_metrics = compute_rf_metrics(val_preds, val_labels)\n",
        "test_metrics = compute_rf_metrics(test_preds, test_labels)\n",
        "\n",
        "print(\"\\nValidation Set Evaluation:\")\n",
        "for metric, value in val_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nTest Set Evaluation:\")\n",
        "for metric, value in test_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, test_preds))\n",
        "\n",
        "joblib.dump(rf_model, \"random_forest_propaganda.pkl\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "print(\"‚úÖ Random Forest Model Training Complete! Model and Vectorizer Saved.\")\n"
      ],
      "metadata": {
        "id": "Nhe5QPDqEBR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from evaluate import load\n",
        "\n",
        "file_path = \"full_labeled_comments_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    import re\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Processed_Text\"] = df[\"Comment Text\"].astype(str).apply(preprocess_text)\n",
        "\n",
        "def integrate_features(row):\n",
        "    central = \"Central Cue: Yes.\" if row[\"Central Cue\"] == 1 else \"Central Cue: No.\"\n",
        "    peripheral = \"Peripheral Cue: Yes.\" if row[\"Peripheral Cue\"] == 1 else \"Peripheral Cue: No.\"\n",
        "    return f\"{central} {peripheral} {row['Processed_Text']}\"\n",
        "\n",
        "df[\"Final_Text\"] = df.apply(integrate_features, axis=1)\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"Final_Text\"].tolist(), df[\"Propaganda\"].tolist(),\n",
        "    test_size=0.3, stratify=df[\"Propaganda\"], random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels,\n",
        "    test_size=0.5, stratify=temp_labels, random_state=42\n",
        ")\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "val_encodings = tokenize_function(val_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "class PropagandaDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = PropagandaDataset(train_encodings, train_labels)\n",
        "val_dataset = PropagandaDataset(val_encodings, val_labels)\n",
        "test_dataset = PropagandaDataset(test_encodings, test_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    eval_steps=50,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    learning_rate=0.00001,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "train_labels_np = np.array(train_labels)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels_np),\n",
        "    y=train_labels_np\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "accuracy_metric = load(\"accuracy\")\n",
        "precision_metric = load(\"precision\")\n",
        "recall_metric = load(\"recall\")\n",
        "f1_metric = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=None)[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=None)[\"recall\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=None)[\"f1\"]\n",
        "\n",
        "    precision_avg = np.mean(precision).item()\n",
        "    recall_avg = np.mean(recall).item()\n",
        "    f1_avg = np.mean(f1).item()\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"precision\": precision_avg,\n",
        "        \"recall\": recall_avg,\n",
        "        \"f1_score\": f1_avg\n",
        "    }\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"\\nüîç Test Set Evaluation Results:\")\n",
        "for metric, value in test_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "model.save_pretrained(\"arabert_propaganda_model_comments_with_cues\")\n",
        "tokenizer.save_pretrained(\"arabert_propaganda_model_comments_with_cues\")\n",
        "\n",
        "print(\"\\n‚úÖ Model Training and Evaluation Complete! Model Saved.\")\n"
      ],
      "metadata": {
        "id": "xEjgR12intis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from evaluate import load\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "file_path = \"full_labeled_comments_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Processed_Text\"] = df[\"Comment Text\"].astype(str).apply(preprocess_text)\n",
        "\n",
        "def integrate_features(row):\n",
        "    central = \"Central Cue: Yes.\" if row[\"Central Cue\"] == 1 else \"Central Cue: No.\"\n",
        "    peripheral = \"Peripheral Cue: Yes.\" if row[\"Peripheral Cue\"] == 1 else \"Peripheral Cue: No.\"\n",
        "    return f\"{central} {peripheral} {row['Processed_Text']}\"\n",
        "\n",
        "df[\"Final_Text\"] = df.apply(integrate_features, axis=1)\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"Final_Text\"].tolist(), df[\"Propaganda\"].tolist(),\n",
        "    test_size=0.3, stratify=df[\"Propaganda\"], random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels,\n",
        "    test_size=0.5, stratify=temp_labels, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "train_features = vectorizer.fit_transform(train_texts)\n",
        "val_features = vectorizer.transform(val_texts)\n",
        "test_features = vectorizer.transform(test_texts)\n",
        "\n",
        "train_labels_np = np.array(train_labels)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels_np),\n",
        "    y=train_labels_np\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=20,\n",
        "    class_weight=class_weights_dict,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(train_features, train_labels)\n",
        "\n",
        "val_preds = rf_model.predict(val_features)\n",
        "test_preds = rf_model.predict(test_features)\n",
        "\n",
        "def compute_rf_metrics(preds, labels):\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=None).mean(),\n",
        "        \"recall\": recall_score(labels, preds, average=None).mean(),\n",
        "        \"f1_score\": f1_score(labels, preds, average=None).mean()\n",
        "    }\n",
        "\n",
        "val_metrics = compute_rf_metrics(val_preds, val_labels)\n",
        "test_metrics = compute_rf_metrics(test_preds, test_labels)\n",
        "\n",
        "print(\"\\nValidation Set Evaluation:\")\n",
        "for metric, value in val_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nTest Set Evaluation:\")\n",
        "for metric, value in test_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, test_preds))\n",
        "\n",
        "joblib.dump(rf_model, \"random_forest_propaganda_comments.pkl\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer_comments.pkl\")\n",
        "print(\"‚úÖ Random Forest Model Training Complete! Model and Vectorizer Saved.\")"
      ],
      "metadata": {
        "id": "7rL8jOFNqcFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_path = \"arabert_propaganda_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "unlabeled_file = \"formatted_comments_dataset.xlsx\"\n",
        "df_unlabeled = pd.read_excel(unlabeled_file)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    import re\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
        "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
        "    text = re.sub(r'Ÿä', 'Ÿâ', text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df_unlabeled[\"Processed_Text\"] = df_unlabeled[\"Comment Text\"].apply(preprocess_text)\n",
        "\n",
        "def predict_propaganda(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "    return prediction\n",
        "\n",
        "df_unlabeled[\"Predicted_Propaganda\"] = df_unlabeled[\"Processed_Text\"].apply(predict_propaganda)\n",
        "\n",
        "df_unlabeled.to_excel(\"predicted_propaganda_comments_data.xlsx\", index=False)\n",
        "\n",
        "print(\"‚úÖ Prediction Complete! Saved results to 'predicted_propaganda_data.xlsx'\")\n"
      ],
      "metadata": {
        "id": "ySD5ZlV2H8IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "posts_file_path = \"full_features_labeled_posts.xlsx\"\n",
        "comments_file_path = \"full_labeled_comments_data.xlsx\"\n",
        "\n",
        "df_posts = pd.read_excel(posts_file_path)\n",
        "df_comments = pd.read_excel(comments_file_path)\n",
        "\n",
        "X_posts = df_posts[[\"Central Cue\", \"Peripheral Cue\"]]\n",
        "y_posts = df_posts[\"Propaganda\"]\n",
        "\n",
        "X_comments = df_comments[[\"Central Cue\", \"Peripheral Cue\"]]\n",
        "y_comments = df_comments[\"Propaganda\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_posts_scaled = scaler.fit_transform(X_posts)\n",
        "X_comments_scaled = scaler.fit_transform(X_comments)\n",
        "\n",
        "log_reg_posts = LogisticRegression()\n",
        "log_reg_comments = LogisticRegression()\n",
        "\n",
        "log_reg_posts.fit(X_posts_scaled, y_posts)\n",
        "log_reg_comments.fit(X_comments_scaled, y_comments)\n",
        "\n",
        "log_reg_coeff_posts = log_reg_posts.coef_[0]\n",
        "log_reg_coeff_comments = log_reg_comments.coef_[0]\n",
        "\n",
        "features = [\"Central Cue\", \"Peripheral Cue\"]\n",
        "\n",
        "log_reg_values_posts = [log_reg_coeff_posts[0], log_reg_coeff_posts[1]]\n",
        "log_reg_values_comments = [log_reg_coeff_comments[0], log_reg_coeff_comments[1]]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(features, log_reg_values_posts, color=['blue', 'orange'])\n",
        "plt.title(\"Logistic Regression - Posts\")\n",
        "plt.ylabel(\"Coefficient Value\")\n",
        "plt.ylim(-0.5, 0.5)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(features, log_reg_values_comments, color=['blue', 'orange'])\n",
        "plt.title(\"Logistic Regression - Comments\")\n",
        "plt.ylim(-0.5, 0.5)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dp26bTFmsXTd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}