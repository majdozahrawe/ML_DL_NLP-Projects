{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !zip -r dataset.zip dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jZWIjRT6ghvo"
      },
      "id": "jZWIjRT6ghvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r dataset"
      ],
      "metadata": {
        "id": "0ccOQz-4R6kf"
      },
      "id": "0ccOQz-4R6kf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip dataset.zip -d ''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oXI9o2tX0MEo"
      },
      "id": "oXI9o2tX0MEo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# import random\n",
        "\n",
        "# def split_train_val(train_folder, val_folder, val_ratio=0.15):\n",
        "#     os.makedirs(val_folder, exist_ok=True)\n",
        "\n",
        "#     base_filenames = set(os.path.splitext(f)[0] for f in os.listdir(train_folder))\n",
        "\n",
        "#     base_filenames = list(base_filenames)\n",
        "#     random.shuffle(base_filenames)\n",
        "#     val_size = int(len(base_filenames) * val_ratio)\n",
        "#     val_filenames = base_filenames[:val_size]\n",
        "\n",
        "#     for base_name in val_filenames:\n",
        "#         for ext in ['.xml', '.txt', '.jpeg', '.png']:\n",
        "#             train_file = os.path.join(train_folder, base_name + ext)\n",
        "#             if os.path.exists(train_file):\n",
        "#                 shutil.move(train_file, os.path.join(val_folder, base_name + ext))\n",
        "\n",
        "# train_folder = '/content/dataset/License-Characters-by-2-27classes/train'\n",
        "# val_folder = '/content/dataset/License-Characters-by-2-27classes/val'\n",
        "# split_train_val(train_folder, val_folder, val_ratio=0.15)\n"
      ],
      "metadata": {
        "id": "-7-zlcwE33eE"
      },
      "id": "-7-zlcwE33eE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb680c20-229e-407b-b2f8-18f3e4375321",
      "metadata": {
        "id": "eb680c20-229e-407b-b2f8-18f3e4375321"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import json\n",
        "# from xml.etree import ElementTree\n",
        "\n",
        "# def xml_to_coco(xml_files_directory, output_json_path):\n",
        "#     coco_format = {\n",
        "#         \"images\": [],\n",
        "#         \"annotations\": [],\n",
        "#         \"categories\": []\n",
        "#     }\n",
        "#     category_set = set()\n",
        "#     annotation_id = 1\n",
        "\n",
        "#     for xml_file in os.listdir(xml_files_directory):\n",
        "#         if xml_file.endswith('.xml'):\n",
        "#             tree = ElementTree.parse(os.path.join(xml_files_directory, xml_file))\n",
        "#             root = tree.getroot()\n",
        "\n",
        "#             file_name = root.find('filename').text\n",
        "#             width = int(root.find('size/width').text)\n",
        "#             height = int(root.find('size/height').text)\n",
        "\n",
        "#             image_id = len(coco_format[\"images\"]) + 1\n",
        "#             coco_format[\"images\"].append({\n",
        "#                 \"file_name\": file_name,\n",
        "#                 \"height\": height,\n",
        "#                 \"width\": width,\n",
        "#                 \"id\": image_id\n",
        "#             })\n",
        "\n",
        "#             for obj in root.findall('object'):\n",
        "#                 category = obj.find('name').text\n",
        "#                 if category not in category_set:\n",
        "#                     category_set.add(category)\n",
        "#                     coco_format[\"categories\"].append({\n",
        "#                         \"id\": len(coco_format[\"categories\"]) + 1,\n",
        "#                         \"name\": category\n",
        "#                     })\n",
        "\n",
        "#                 category_id = next((cat[\"id\"] for cat in coco_format[\"categories\"] if cat[\"name\"] == category), None)\n",
        "#                 bndbox = obj.find('bndbox')\n",
        "#                 xmin = int(bndbox.find('xmin').text)\n",
        "#                 ymin = int(bndbox.find('ymin').text)\n",
        "#                 xmax = int(bndbox.find('xmax').text)\n",
        "#                 ymax = int(bndbox.find('ymax').text)\n",
        "\n",
        "#                 coco_format[\"annotations\"].append({\n",
        "#                     \"id\": annotation_id,\n",
        "#                     \"image_id\": image_id,\n",
        "#                     \"category_id\": category_id,\n",
        "#                     \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin],\n",
        "#                     \"area\": (xmax - xmin) * (ymax - ymin),\n",
        "#                     \"segmentation\": [],\n",
        "#                     \"iscrowd\": 0\n",
        "#                 })\n",
        "#                 annotation_id += 1\n",
        "\n",
        "#     with open(output_json_path, 'w') as f:\n",
        "#         json.dump(coco_format, f, indent=4)\n",
        "\n",
        "# xml_to_coco(xml_files_directory='/content/dataset/License-Characters-by-2-27classes/train', output_json_path='/content/dataset/License-Characters-by-2-27classes/train/_coco_annotation_train.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171b763b-ce10-41b9-a7ba-fc59753144b2",
      "metadata": {
        "id": "171b763b-ce10-41b9-a7ba-fc59753144b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import torchvision.ops\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import functional as FT\n",
        "from torchvision import transforms as T\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b21e11-0146-49cf-8960-2072c51ea29c",
      "metadata": {
        "id": "c6b21e11-0146-49cf-8960-2072c51ea29c"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf50c3e1-49c7-4542-ade4-86e66f54b51d",
      "metadata": {
        "id": "bf50c3e1-49c7-4542-ade4-86e66f54b51d"
      },
      "outputs": [],
      "source": [
        "def get_transforms(train=False):\n",
        "    if train:\n",
        "        transform = A.Compose([\n",
        "            A.Resize(640, 640),\n",
        "            A.HorizontalFlip(p=0.3),\n",
        "            A.VerticalFlip(p=0.3),\n",
        "            A.RandomBrightnessContrast(p=0.1),\n",
        "            A.ColorJitter(p=0.1),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='coco'))\n",
        "    else:\n",
        "        transform = A.Compose([\n",
        "            A.Resize(640, 640),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='coco'))\n",
        "    return transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab674b7d-a81f-49d7-b4b7-21fe17f59717",
      "metadata": {
        "id": "ab674b7d-a81f-49d7-b4b7-21fe17f59717"
      },
      "outputs": [],
      "source": [
        "class License_Plate_Detection(datasets.VisionDataset):\n",
        "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "        self.split = split\n",
        "        self.coco = COCO(os.path.join(root, split, \"_coco_annotation_train.json\"))\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
        "\n",
        "    def _load_image(self, id: int):\n",
        "        path = self.coco.loadImgs(id)[0]['file_name']\n",
        "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        return image\n",
        "\n",
        "    def _load_target(self, id):\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        target = self._load_target(id)\n",
        "        target = copy.deepcopy(self._load_target(id))\n",
        "\n",
        "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image, bboxes=boxes)\n",
        "\n",
        "        image = transformed['image']\n",
        "        boxes = transformed['bboxes']\n",
        "\n",
        "        new_boxes = []\n",
        "        for box in boxes:\n",
        "            xmin = box[0]\n",
        "            xmax = xmin + box[2]\n",
        "            ymin = box[1]\n",
        "            ymax = ymin + box[3]\n",
        "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
        "        category_id_to_index = {cat_id: idx for idx, (cat_id, cat_info) in enumerate(categories.items())}\n",
        "\n",
        "        targ = {}\n",
        "        targ['boxes'] = boxes\n",
        "        targ['labels'] = torch.tensor([category_id_to_index[t['category_id']] for t in target], dtype=torch.int64)\n",
        "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
        "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
        "        return image.div(255), targ\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92779552-5810-4dbb-9bc9-6164b8d81e3d",
      "metadata": {
        "id": "92779552-5810-4dbb-9bc9-6164b8d81e3d"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/dataset/License-Characters-by-2-27classes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af1b400-33e0-4c09-ba5b-9e117f1e68ed",
      "metadata": {
        "scrolled": true,
        "id": "9af1b400-33e0-4c09-ba5b-9e117f1e68ed"
      },
      "outputs": [],
      "source": [
        "coco = COCO(os.path.join(dataset_path, \"train\", \"_coco_annotation_train.json\"))\n",
        "categories = coco.cats\n",
        "n_classes = len(categories.keys())\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739821f0-1e78-469e-8e9d-8506c5e93e4b",
      "metadata": {
        "scrolled": true,
        "id": "739821f0-1e78-469e-8e9d-8506c5e93e4b"
      },
      "outputs": [],
      "source": [
        "classes = [i[1]['name'] for i in categories.items()]\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a934a4-29ae-4b4b-87a1-30d9674b4cfc",
      "metadata": {
        "id": "28a934a4-29ae-4b4b-87a1-30d9674b4cfc"
      },
      "outputs": [],
      "source": [
        "train_dataset = License_Plate_Detection(root=dataset_path, transforms=get_transforms(True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = License_Plate_Detection(root=dataset_path, split=\"val\", transforms=get_transforms(False))"
      ],
      "metadata": {
        "id": "42_Bfu_o5VMf"
      },
      "id": "42_Bfu_o5VMf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8407f5-23e7-4852-87a3-359d6c4e6b3d",
      "metadata": {
        "id": "0f8407f5-23e7-4852-87a3-359d6c4e6b3d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "sample = train_dataset[32]\n",
        "img_tensor = sample[0] * 255\n",
        "img_int = img_tensor.to(torch.uint8)\n",
        "\n",
        "new_width, new_height = img_tensor.shape[2] * 2, img_tensor.shape[1] * 2\n",
        "img_resized = F.resize(img_int, [new_height, new_width])\n",
        "\n",
        "boxes_resized = sample[1]['boxes'] * torch.tensor([2, 2, 2, 2], dtype=torch.float32)\n",
        "\n",
        "img_with_boxes = draw_bounding_boxes(\n",
        "    img_resized, boxes_resized, [classes[i] for i in sample[1]['labels']], width=6\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img_with_boxes.permute(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7918660-c7f7-4f93-9530-5d688225251f",
      "metadata": {
        "id": "c7918660-c7f7-4f93-9530-5d688225251f"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15844f51-0231-4c63-a036-f06ea0c4cb04",
      "metadata": {
        "id": "15844f51-0231-4c63-a036-f06ea0c4cb04"
      },
      "outputs": [],
      "source": [
        "# # model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# # in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# # model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
        "\n",
        "# model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "# in_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\n",
        "# model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
        "\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "def create_faster_rcnn_resnet34(num_classes):\n",
        "    backbone = resnet_fpn_backbone('resnet34', pretrained=True)\n",
        "    model = FasterRCNN(backbone, num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "model = create_faster_rcnn_resnet34(num_classes=27)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7872e3e8-b064-4b41-80b2-ab1e12ad0589",
      "metadata": {
        "id": "7872e3e8-b064-4b41-80b2-ab1e12ad0589"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542d591b-bd5d-48bd-ac36-e30835050611",
      "metadata": {
        "id": "542d591b-bd5d-48bd-ac36-e30835050611"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2,  collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "GCj9Px-_8mE_"
      },
      "id": "GCj9Px-_8mE_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ce870d-5bf9-446d-8da5-0efe8ae43755",
      "metadata": {
        "id": "81ce870d-5bf9-446d-8da5-0efe8ae43755"
      },
      "outputs": [],
      "source": [
        "images,targets = next(iter(train_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k:v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed77147f-37c9-4edb-911f-b85e4f5e4af7",
      "metadata": {
        "id": "ed77147f-37c9-4edb-911f-b85e4f5e4af7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e204d4c9-a176-4872-b936-da40c0fac650",
      "metadata": {
        "id": "e204d4c9-a176-4872-b936-da40c0fac650"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e2b37a-9d7c-4a8a-8fa2-76dff64e63a7",
      "metadata": {
        "id": "06e2b37a-9d7c-4a8a-8fa2-76dff64e63a7"
      },
      "outputs": [],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1) # lr scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c836cd04-fb21-47c0-8715-eb744ae4d27c",
      "metadata": {
        "id": "c836cd04-fb21-47c0-8715-eb744ae4d27c"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8519791-1934-48d6-aa06-d7e66296a6dc",
      "metadata": {
        "id": "f8519791-1934-48d6-aa06-d7e66296a6dc"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    all_losses = []\n",
        "    all_losses_dict = []\n",
        "\n",
        "    for images, targets in tqdm(loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        all_losses.append(loss_value)\n",
        "        all_losses_dict.append(loss_dict_append)\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "\n",
        "    all_losses_dict = pd.DataFrame(all_losses_dict)\n",
        "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
        "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
        "        all_losses_dict['loss_classifier'].mean(),\n",
        "        all_losses_dict['loss_box_reg'].mean(),\n",
        "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
        "        all_losses_dict['loss_objectness'].mean()\n",
        "    ))\n",
        "\n",
        "    return all_losses, all_losses_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def iou(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x1g, y1g, x2g, y2g = box2\n",
        "\n",
        "    xi1, yi1 = max(x1, x1g), max(y1, y1g)\n",
        "    xi2, yi2 = min(x2, x2g), min(y2, y2g)\n",
        "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
        "\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "def calculate_precision_recall(all_boxes, all_scores, all_labels, true_boxes, true_labels, iou_threshold=0.5):\n",
        "\n",
        "    sorted_indices = np.argsort(-np.array(all_scores))\n",
        "    all_boxes = np.array(all_boxes)[sorted_indices]\n",
        "    all_labels = np.array(all_labels)[sorted_indices]\n",
        "    all_scores = np.array(all_scores)[sorted_indices]\n",
        "\n",
        "    num_true_boxes = len(true_boxes)\n",
        "    num_pred_boxes = len(all_boxes)\n",
        "\n",
        "    true_positive = np.zeros(num_pred_boxes)\n",
        "    false_positive = np.zeros(num_pred_boxes)\n",
        "    detected_boxes = []\n",
        "\n",
        "    for idx, pred_box in enumerate(all_boxes):\n",
        "        max_iou = 0\n",
        "        max_idx = -1\n",
        "        for jdx, true_box in enumerate(true_boxes):\n",
        "            if all_labels[idx] == true_labels[jdx] and jdx not in detected_boxes:\n",
        "                current_iou = iou(pred_box, true_box)\n",
        "                if current_iou > max_iou:\n",
        "                    max_iou = current_iou\n",
        "                    max_idx = jdx\n",
        "\n",
        "        if max_iou >= iou_threshold:\n",
        "            true_positive[idx] = 1\n",
        "            detected_boxes.append(max_idx)\n",
        "        else:\n",
        "            false_positive[idx] = 1\n",
        "\n",
        "    cum_tp = np.cumsum(true_positive)\n",
        "    cum_fp = np.cumsum(false_positive)\n",
        "    precision = cum_tp / (cum_tp + cum_fp)\n",
        "    recall = cum_tp / num_true_boxes\n",
        "\n",
        "    return precision, recall, all_scores\n",
        "\n",
        "def calculate_ap(precision, recall):\n",
        "    recall = np.concatenate(([0.0], recall, [1.0]))\n",
        "    precision = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    for i in range(len(precision) - 2, -1, -1):\n",
        "        precision[i] = max(precision[i], precision[i + 1])\n",
        "\n",
        "    indices = np.where(recall[1:] != recall[:-1])[0]\n",
        "    average_precision = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
        "\n",
        "    return average_precision\n",
        "\n",
        "def compute_map(dataset, model, device, iou_thresh=0.5):\n",
        "\n",
        "    all_detections = []\n",
        "    all_annotations = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataset:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                scores = output['scores'].cpu().numpy()\n",
        "                labels = output['labels'].cpu().numpy()\n",
        "                boxes = output['boxes'].cpu().numpy()\n",
        "\n",
        "                target_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "                target_labels = targets[i]['labels'].cpu().numpy()\n",
        "\n",
        "                valid = scores > 0.5\n",
        "\n",
        "                if len(valid) > 0 and valid.any():\n",
        "                    scores = scores[valid]\n",
        "                    labels = labels[valid]\n",
        "                    boxes = boxes[valid]\n",
        "\n",
        "                    all_detections.extend([(box, score, label) for box, score, label in zip(boxes, scores, labels)])\n",
        "\n",
        "                all_annotations.extend([(box, label) for box, label in zip(target_boxes, target_labels)])\n",
        "\n",
        "    if not all_detections:\n",
        "        print(\"No detections found. Returning 0 for mAP.\")\n",
        "        return 0\n",
        "\n",
        "    pred_boxes, pred_scores, pred_labels = zip(*all_detections)\n",
        "    true_boxes, true_labels = zip(*all_annotations)\n",
        "\n",
        "    aps = []\n",
        "    for class_id in np.unique(true_labels):\n",
        "        class_pred_boxes = [pred_boxes[i] for i in range(len(pred_labels)) if pred_labels[i] == class_id]\n",
        "        class_pred_scores = [pred_scores[i] for i in range(len(pred_labels)) if pred_labels[i] == class_id]\n",
        "        class_true_boxes = [true_boxes[i] for i in range(len(true_labels)) if true_labels[i] == class_id]\n",
        "\n",
        "        precision, recall, _ = calculate_precision_recall(class_pred_boxes, class_pred_scores, [class_id]*len(class_pred_boxes), class_true_boxes, [class_id]*len(class_true_boxes), iou_threshold=iou_thresh)\n",
        "        ap = calculate_ap(precision, recall)\n",
        "        aps.append(ap)\n",
        "\n",
        "    mean_ap = np.mean(aps)\n",
        "    return mean_ap\n"
      ],
      "metadata": {
        "id": "Q_eRTxtF-yA5"
      },
      "id": "Q_eRTxtF-yA5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def iou(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x1g, y1g, x2g, y2g = box2\n",
        "\n",
        "    xi1, yi1 = max(x1, x1g), max(y1, y1g)\n",
        "    xi2, yi2 = min(x2, x2g), min(y2, y2g)\n",
        "    inter_area = max(xi2 - xi1, 0) * max(yi1 - yi2, 0)\n",
        "\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "def calculate_precision_recall(all_boxes, all_scores, all_labels, true_boxes, true_labels, iou_threshold=0.5):\n",
        "    if len(all_scores) == 0:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    sorted_indices = np.argsort(-np.array(all_scores))\n",
        "    all_boxes = np.array(all_boxes)[sorted_indices]\n",
        "    all_labels = np.array(all_labels)[sorted_indices]\n",
        "    all_scores = np.array(all_scores)[sorted_indices]\n",
        "\n",
        "    num_true_boxes = len(true_boxes)\n",
        "    num_pred_boxes = len(all_boxes)\n",
        "\n",
        "    true_positive = np.zeros(num_pred_boxes)\n",
        "    false_positive = np.zeros(num_pred_boxes)\n",
        "    detected_boxes = []\n",
        "\n",
        "    for idx, pred_box in enumerate(all_boxes):\n",
        "        max_iou = 0\n",
        "        max_idx = -1\n",
        "        for jdx, true_box in enumerate(true_boxes):\n",
        "            if all_labels[idx] == true_labels[jdx] and jdx not in detected_boxes:\n",
        "                current_iou = iou(pred_box, true_box)\n",
        "                if current_iou > max_iou:\n",
        "                    max_iou = current_iou\n",
        "                    max_idx = jdx\n",
        "\n",
        "        if max_iou >= iou_threshold:\n",
        "            true_positive[idx] = 1\n",
        "            detected_boxes.append(max_idx)\n",
        "        else:\n",
        "            false_positive[idx] = 1\n",
        "\n",
        "    cum_tp = np.cumsum(true_positive)\n",
        "    cum_fp = np.cumsum(false_positive)\n",
        "    precision = cum_tp / (cum_tp + cum_fp) if (cum_tp + cum_fp).sum() > 0 else np.array([])\n",
        "    recall = cum_tp / num_true_boxes if num_true_boxes > 0 else np.array([])\n",
        "\n",
        "    return precision, recall, all_scores\n",
        "\n",
        "def calculate_ap(precision, recall):\n",
        "    if len(precision) == 0 or len(recall) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    recall = np.concatenate(([0.0], recall, [1.0]))\n",
        "    precision = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    for i in range(len(precision) - 2, -1, -1):\n",
        "        precision[i] = max(precision[i], precision[i + 1])\n",
        "\n",
        "    indices = np.where(recall[1:] != recall[:-1])[0]\n",
        "    average_precision = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
        "\n",
        "    return average_precision\n",
        "\n",
        "def evaluate_model(loader, model, device, class_names, iou_thresh=0.5):\n",
        "    model.eval()\n",
        "    all_detections = []\n",
        "    all_annotations = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                scores = output['scores'].cpu().numpy()\n",
        "                labels = output['labels'].cpu().numpy()\n",
        "                boxes = output['boxes'].cpu().numpy()\n",
        "\n",
        "                target_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "                target_labels = targets[i]['labels'].cpu().numpy()\n",
        "\n",
        "                valid = scores > 0.5\n",
        "                scores = scores[valid]\n",
        "                labels = labels[valid]\n",
        "                boxes = boxes[valid]\n",
        "\n",
        "                all_detections.extend([(box, score, label) for box, score, label in zip(boxes, scores, labels)])\n",
        "                all_annotations.extend([(box, label) for box, label in zip(target_boxes, target_labels)])\n",
        "\n",
        "    if not all_detections:\n",
        "        return [], [], [], 0\n",
        "\n",
        "    pred_boxes, pred_scores, pred_labels = zip(*all_detections)\n",
        "    true_boxes, true_labels = zip(*all_annotations)\n",
        "\n",
        "    aps, precisions, recalls = [], [], []\n",
        "\n",
        "    for class_id in range(1, 27):\n",
        "        class_pred_boxes = [pred_boxes[i] for i in range(len(pred_labels)) if pred_labels[i] == class_id]\n",
        "        class_pred_scores = [pred_scores[i] for i in range(len(pred_labels)) if pred_labels[i] == class_id]\n",
        "        class_true_boxes = [true_boxes[i] for i in range(len(true_labels)) if true_labels[i] == class_id]\n",
        "\n",
        "        precision, recall, _ = calculate_precision_recall(class_pred_boxes, class_pred_scores, [class_id]*len(class_pred_boxes), class_true_boxes, [class_id]*len(class_true_boxes), iou_threshold=iou_thresh)\n",
        "        ap = calculate_ap(precision, recall)\n",
        "\n",
        "        if len(precision) == 0:\n",
        "            print(f\"No predictions for {class_names[class_id-1]}.\")\n",
        "        if len(recall) == 0:\n",
        "            print(f\"No true positives for {class_names[class_id-1]}.\")\n",
        "\n",
        "        aps.append(ap)\n",
        "        precisions.append(np.nanmean(precision) if len(precision) > 0 else float('nan'))\n",
        "        recalls.append(np.nanmean(recall) if len(recall) > 0 else float('nan'))\n",
        "\n",
        "    mean_ap = np.mean(aps)\n",
        "    return precisions, recalls, aps, mean_ap\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "epoch_losses = []\n",
        "epoch_losses_dict = []\n",
        "\n",
        "total_start_time = time.time()\n",
        "\n",
        "train_mAPs, val_mAPs = [], []\n",
        "train_precisions, val_precisions = [], []\n",
        "train_recalls, val_recalls = [], []\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    losses, losses_dict = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    train_precisions_epoch, train_recalls_epoch, train_aps, train_mean_ap = evaluate_model(train_loader, model, device, classes)\n",
        "    train_mAPs.append(train_mean_ap)\n",
        "    train_precisions.append(np.nanmean(train_precisions_epoch))\n",
        "    train_recalls.append(np.nanmean(train_recalls_epoch))\n",
        "    print(f\"Epoch {epoch} - Train mAP@0.50: {train_mean_ap}\")\n",
        "    # for class_id, (precision, recall, ap) in enumerate(zip(train_precisions_epoch, train_recalls_epoch, train_aps), start=1):\n",
        "    #     print(f\"{classes[class_id-1]} - Precision: {precision}, Recall: {recall}, AP: {ap}\")\n",
        "\n",
        "    val_precisions_epoch, val_recalls_epoch, val_aps, val_mean_ap = evaluate_model(val_loader, model, device, classes)\n",
        "    val_mAPs.append(val_mean_ap)\n",
        "    val_precisions.append(np.nanmean(val_precisions_epoch))\n",
        "    val_recalls.append(np.nanmean(val_recalls_epoch))\n",
        "    print(f\"Epoch {epoch} - Validation mAP@0.50: {val_mean_ap}\")\n",
        "    # for class_id, (precision, recall, ap) in enumerate(zip(val_precisions_epoch, val_recalls_epoch, val_aps), start=1):\n",
        "    #     print(f\"{classes[class_id-1]} - Precision: {precision}, Recall: {recall}, AP: {ap}\")\n",
        "\n",
        "    epoch_losses.append(np.mean(losses))\n",
        "    epoch_losses_dict.append(losses_dict.mean())\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "print(f\"Total training time: {total_duration:.2f} seconds\")\n",
        "\n",
        "epochs = range(num_epochs)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, train_mAPs, label='Train mAP')\n",
        "plt.plot(epochs, val_mAPs, label='Validation mAP')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('mAP')\n",
        "plt.title('mAP over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, train_precisions, label='Train Precision')\n",
        "plt.plot(epochs, val_precisions, label='Validation Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs, train_recalls, label='Train Recall')\n",
        "plt.plot(epochs, val_recalls, label='Validation Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "train_f1_scores = [2 * (p * r) / (p + r) for p, r in zip(train_precisions, train_recalls)]\n",
        "val_f1_scores = [2 * (p * r) / (p + r) for p, r in zip(val_precisions, val_recalls)]\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs, train_f1_scores, label='Train F1 Score')\n",
        "plt.plot(epochs, val_f1_scores, label='Validation F1 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "epoch_losses_df = pd.DataFrame(epoch_losses_dict)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(num_epochs), epoch_losses, label='Total Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_classifier'], label='Loss Classifier')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_box_reg'], label='Loss Box Reg')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_rpn_box_reg'], label='Loss RPN Box Reg')\n",
        "plt.plot(range(num_epochs), epoch_losses_df['loss_objectness'], label='Loss Objectness')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Individual Loss Components Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kwfO_HGFvstH",
        "collapsed": true
      },
      "id": "kwfO_HGFvstH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9dd95ba-e270-41b1-a781-7797a62de4f2",
      "metadata": {
        "collapsed": true,
        "id": "d9dd95ba-e270-41b1-a781-7797a62de4f2"
      },
      "outputs": [],
      "source": [
        "num_epochs=40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    train_map50 = compute_map(train_loader, model, device)\n",
        "    val_map50 = compute_map(val_loader, model, device)\n",
        "\n",
        "    print(f\"Train mAP@0.50: {train_map50}\")\n",
        "    print(f\"Validation mAP@0.50: {val_map50}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "id": "0RywainD4K0U"
      },
      "id": "0RywainD4K0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd2f5eef-6a79-4320-b121-6c0ec93a73e9",
      "metadata": {
        "id": "bd2f5eef-6a79-4320-b121-6c0ec93a73e9"
      },
      "outputs": [],
      "source": [
        "test_dataset = License_Plate_Detection(root=dataset_path, split=\"test\", transforms=get_transforms(False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6edea44c-d14f-4de1-abc3-dd45eaba9c65",
      "metadata": {
        "id": "6edea44c-d14f-4de1-abc3-dd45eaba9c65"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,  collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d691562b-1576-4d71-a5f8-a090cc223814",
      "metadata": {
        "id": "d691562b-1576-4d71-a5f8-a090cc223814"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f34fa40-c949-4bd2-9a98-c47edea6169b",
      "metadata": {
        "id": "4f34fa40-c949-4bd2-9a98-c47edea6169b"
      },
      "outputs": [],
      "source": [
        "img, _ = test_dataset[2]\n",
        "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])\n",
        "    pred = prediction[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f83540cb-40d4-4aa4-bdd4-f9b24bd280cf",
      "metadata": {
        "id": "f83540cb-40d4-4aa4-bdd4-f9b24bd280cf"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(14, 10))\n",
        "plt.imshow(draw_bounding_boxes(img_int,\n",
        "    pred['boxes'][pred['scores'] > 0.5],\n",
        "    [classes[i] for i in pred['labels'][pred['scores'] > 0.5].tolist()], width=4\n",
        ").permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570240bc-cf2b-40a1-958d-4280b1c3bbdc",
      "metadata": {
        "id": "570240bc-cf2b-40a1-958d-4280b1c3bbdc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}